{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using pretrained, well-known models\n",
    "This notebook aims to create a set of benchmarks for the project, using well-known, thoroughly studied models.\n",
    "Either with pretrained weights, or training with new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 2\n",
      "Energies shape: (10000, 2)\n",
      "Positions shape: (10000, 4)\n",
      "Labels shape: (10000, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import master_data_functions.functions as dfunc\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# silence deprecation warnings from tensorflow\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "\n",
    "# File import\n",
    "# Sample filenames are:\n",
    "# CeBr10kSingle_1.txt -> single events, \n",
    "# CeBr10kSingle_2.txt -> single events\n",
    "# CeBr10k_1.txt -> mixed single and double events \n",
    "# CeBr10.txt -> small file of 10 samples\n",
    "\n",
    "keywords = {\n",
    "    \"single_1\": \"CeBr10kSingle_1\",\n",
    "    \"single_2\": \"CeBr10kSingle_2\",\n",
    "    \"mix\": \"CeBr10k_1\",\n",
    "    \"small\": \"CeBr10\"}\n",
    "    \n",
    "dataset = \"mix\"\n",
    "data = dfunc.import_data(\"sample\")\n",
    "images = data[keywords[dataset]][\"images\"]\n",
    "energies = data[keywords[dataset]][\"energies\"]\n",
    "positions = data[keywords[dataset]][\"positions\"]\n",
    "labels = to_categorical(data[keywords[dataset]][\"labels\"])\n",
    "n_classes = labels.shape[1]\n",
    "\n",
    "\n",
    "print(\"Number of classes: {}\".format(n_classes))\n",
    "print(\"Energies shape: {}\".format(energies.shape))\n",
    "print(\"Positions shape: {}\".format(positions.shape))\n",
    "print(\"Labels shape: {}\".format(labels.shape))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image data shape: (10000, 16, 16, 3)\n"
     ]
    }
   ],
   "source": [
    "# VGG16 expects 3 channels. Solving this by concatenating the image data \n",
    "# to itself, to form three identical channels\n",
    "\n",
    "images = np.concatenate((images, images, images), axis=3)\n",
    "\n",
    "print(\"Image data shape: {}\".format(images.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Set up training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and test data shapes:\n",
      "x_train: (8000, 16, 16, 3)\n",
      "x_test: (2000, 16, 16, 3)\n",
      "y_train: (8000, 2)\n",
      "y_test: (2000, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size = 0.2)\n",
    "print(\"Training and test data shapes:\")\n",
    "print(\"x_train: {}\".format(x_train.shape))\n",
    "print(\"x_test: {}\".format(x_test.shape))\n",
    "print(\"y_train: {}\".format(y_train.shape))\n",
    "print(\"y_test: {}\".format(y_test.shape))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-16\n",
    "Using Keras as our framework with Tensorflow backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0810 17:36:59.730076 140090784458368 deprecation_wrapper.py:119] From /home/ulvik/git/master_analysis/notebooks/pretrained/pretrained_venv/lib64/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0810 17:36:59.761569 140090784458368 deprecation_wrapper.py:119] From /home/ulvik/git/master_analysis/notebooks/pretrained/pretrained_venv/lib64/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0810 17:36:59.768360 140090784458368 deprecation_wrapper.py:119] From /home/ulvik/git/master_analysis/notebooks/pretrained/pretrained_venv/lib64/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0810 17:36:59.853263 140090784458368 deprecation_wrapper.py:119] From /home/ulvik/git/master_analysis/notebooks/pretrained/pretrained_venv/lib64/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0810 17:37:00.424957 140090784458368 deprecation_wrapper.py:119] From /home/ulvik/git/master_analysis/notebooks/pretrained/pretrained_venv/lib64/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0810 17:37:00.426215 140090784458368 deprecation_wrapper.py:119] From /home/ulvik/git/master_analysis/notebooks/pretrained/pretrained_venv/lib64/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "model = VGG16(weights='imagenet', include_top=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile a model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c78356de4d74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/git/master_analysis/notebooks/pretrained/pretrained_venv/lib64/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/master_analysis/notebooks/pretrained/pretrained_venv/lib64/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m                 raise RuntimeError('You must compile a model before '\n\u001b[0m\u001b[1;32m    682\u001b[0m                                    \u001b[0;34m'training/testing. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m                                    'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile a model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "# Parameters for the model\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "model.evaluate(x_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on test-set and plot some examples\n",
    "To compare predicted positions with actual positions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_pos = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plot some images, with electron origin positions\n",
    "%matplotlib inline\n",
    "\n",
    "index = 100\n",
    "fig, ax = plt.subplots(3, 3, sharex='col', sharey='row', figsize=(12,12))\n",
    "\n",
    "# Reshape test-data for plotting\n",
    "x_plot = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2])\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        # plot image\n",
    "        ax[i, j].imshow(x_plot[index + i*3 + j])\n",
    "        \n",
    "        # plot true origin of event\n",
    "        x = y_test[index + i*3 + j][0]\n",
    "        y = y_test[index + i*3 + j][1]\n",
    "        ax[i, j].plot(x, y, 'rx')\n",
    "        \n",
    "        # plot predicted origin of event\n",
    "        x_pred = predicted_pos[index + i*3 + j][0]\n",
    "        y_pred = predicted_pos[index + i*3 + j][1]\n",
    "        ax[i, j].plot(x_pred, y_pred, 'wx')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pretrained_venv",
   "language": "python",
   "name": "pretrained_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
