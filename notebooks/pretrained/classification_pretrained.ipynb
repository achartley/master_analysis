{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using pretrained, well-known models\n",
    "This notebook aims to create a set of benchmarks for the project, using well-known, thoroughly studied models.\n",
    "Either with pretrained weights, or training with new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "### 21.08.19\n",
    "Attempting to classify with VGG has not proven effective yet.\n",
    "Initially, the image data was note scaled at all. Implemented scaling in the\n",
    "import function, using min-max scaling of the value. This preserves the inherent\n",
    "intensity difference between images.\n",
    "\n",
    "The number of layers of VGG16 used is varied between 3 to 9 without noticable\n",
    "difference. Attempting to find out why, by analyzing the extracted features.\n",
    "The idea is that in order to classify, the feature distribution should be\n",
    "different for images containing single and double events.\n",
    "I first attempt this with manual qualitative inspection.\n",
    "\n",
    "Manual, qualitative inspection reveals that the distributions look very similar.\n",
    "Performing a quantitative study using Kolmogorov-Smirnov two-sample test,\n",
    "comparing the distribution for each feature.\n",
    "\n",
    "* For 1 block (depth 3), the pvalue returned from comparisons is 1.0 for all features.\n",
    "* For 2 blocks (depth 6), the pvalue returned from comparisons is 1.0 for all features.\n",
    "* For 3 blocks (depth 10), the pvalue returned from comparisons is 1.0 for all features.\n",
    "* For 4 blocks, (depth 14) the pvalue returned from comparisons is 1.0 for all features.\n",
    "* For 5 blocks, (depth 18) the pvalue returned from comparisons is 1.0 for all features.\n",
    "\n",
    "This indicates that extracting features using vgg16 doesn't work for classification.\n",
    "I still want to confirm that the weights of the vgg layers are the imagenet weights.\n",
    "\n",
    "### 22.08.19\n",
    "Going to use a reference image from imagenet to verify that the vgg-layers behave\n",
    "as expected.\n",
    "* Reference produces very similar feature output as simulated data\n",
    "\n",
    "Rewrote the vgg_model script to be able to import any pretrained model from\n",
    "tensorflow, and extended data import to handle single files (for large file)\n",
    "and possibility to specify number of samples to include.\n",
    "\n",
    "### 23.08.19\n",
    "Import scripts fixed so that array dimensions are correct independent of folder or single\n",
    "file import.\n",
    "\n",
    "Tests on multiple nets with 10k events give same results as for VGG.\n",
    "\n",
    "### 24.08.19\n",
    "Running checks on feature distribution with the full networks and 200k events.\n",
    "With 200k events there are models which from the p-value given by the KS two-sample test\n",
    "should be possible to classify with. Interstingly, VGG16 and VGG19 show a large variance in which\n",
    "features are seemingly drawn from different distributions.\n",
    "\n",
    "Need to run for: NASNetLarge, ResNet50 and Xception, but running into memory problems.\n",
    "\n",
    "### 25.08.19\n",
    "Some NaN values in output features from VGG. When removed, the network trains to acc = 0.9\n",
    "on the features. \n",
    "\n",
    "Implemented save_feature_representation and load_feature_representation.\n",
    "\n",
    "### 04.09.19\n",
    "Storing 200k events to use for generating feature representations for classification.\n",
    "\n",
    "### 09.09.19\n",
    "Previously trained fully connected nets on the feature output from all pretrained networks available\n",
    "at full depth. The fully-connected nets were configured so that the input layer had a number of nodes\n",
    "equal to the number of features output from the convolutional block before it (because the input shape\n",
    "must be defined). After that, two layers of 512 nodes with RELu actication functions, into one\n",
    "2-node layer with softmax.\n",
    "Results:\n",
    "* __DenseNet121__: \n",
    "    Accuracy starts around 0.86 first epoch and rises to just under 0.94 at maximum.\n",
    "    Somewhat unstable.\n",
    "* __DenseNet169__: \n",
    "    Very similar to DenseNet121, but starts at around 0.90\n",
    "* __DenseNet201__: \n",
    "    Very similar to other two DenseNets, but more stable above 0.92 for epoch 4+.\n",
    "* __InceptionResNetV2__: \n",
    "    Starts at 0.84, maxes just above 0.88. A bit zig-zag.\n",
    "* __InceptionV3__: \n",
    "    Similar to V2 above, but starts lower and maxes out higher (0.90). Huge dip at epoch 7,\n",
    "    around same spot as DenseNets.\n",
    "* __MobileNet__: \n",
    "    Didn't learn anything.\n",
    "* __MobileNetV2__: \n",
    "    Didn't learn anything.\n",
    "* __NASNetLarge__: \n",
    "    Starts at 0.88, maxes out at 0.92 ish. Stable nice increase in accuracy with small dip at last epoch.\n",
    "* __NASNetMobile__: \n",
    "    Roughly same as NASNetLarge, but a little more zig-zag in the curve.\n",
    "* __ResNet50__: \n",
    "    Didn't learn anything.\n",
    "* __VGG16__: \n",
    "    A lot of zig-zag, but between 0.88 and just under 0.92 (max), so it's a small span.\n",
    "    Stops zig-zagging on max and becomes pretty much flat.\n",
    "* __VGG19__: \n",
    "    Smaller zig-zag distances, steady increase up to around 0.90 for maximum.\n",
    "* __Xception__: \n",
    "    Starts high (0.88) and slowly rises to a maximum of 0.92, a dip around epoch 6 like many others.\n",
    "    \n",
    "From this I think the nets that didn't weren't able to extract anything from the features can be\n",
    "ignored for a while, to focus on those that performed well. This leaves the following nets as possible\n",
    "points of interest:\n",
    "* DenseNet201 (best of the DenseNets, keep the others in mind)\n",
    "* NASNetLarge (keep NASNetMobile in mind)\n",
    "* VGG16\n",
    "* VGG19\n",
    "* Xception\n",
    "\n",
    "### 15.09.19\n",
    "Simple implementation of distance-checking of double events done.\n",
    "Results indicate that the model struggles more with events that are close together.\n",
    "Specifically, I've looked at the ratio of correctly classified double events where\n",
    "the events are less than 3mm apart.\n",
    "\n",
    "### 16.09.19\n",
    "* Compare feature distributions of single events and double events with distance < 3mm.\n",
    "Plots produced. There is a similar difference between single events and close double events,\n",
    "as between single events and double events in general.\n",
    "\n",
    "* Is the number of close events large enough to be confident in the result of the KS two-sample test?\n",
    "* What does the above result imply?\n",
    "* Is weighting the training data such that close events are weighted higher than others a good idea? Would this introduce a bias?\n",
    "\n",
    "### 18.09.19\n",
    "Producing histograms of correct vs wrong classifications for predictions on double events, specifically.\n",
    "This gives additional insight in how well the models classify events, especially compared with the total\n",
    "accuracy. An example is VGG16: The total accuracy for classification is around 0.88-0.92, but when looking\n",
    "at event with a distance lower than 3mm between them, the ratio of correctly classified close events is \n",
    "around 0.4. Densenet201 full depth has comparable total accuracy, but a ratio for close events at 0.72, \n",
    "which is significantly better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "* Implement functions to save and load trained fully-connected networks\n",
    "* Need a way to compare models where validation and test data is consistent.\n",
    "    * This can be part of a new train_test_split which also creates validation data. Could possibly be solved by simply doing train_test_split twice. first on full data, then on the resulting test-data.\n",
    "* Rewrite this notebook to use the new data functions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from master_data_functions.functions import import_data,save_feature_representation,load_feature_representation\n",
    "from master_models.pretrained import pretrained_model\n",
    "from master_data_functions.functions import event_indices, relative_distance, relative_energy\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 2\n",
      "Images shape: (200000, 16, 16, 1)\n",
      "Energies shape: (200000, 2)\n",
      "Positions shape: (200000, 4)\n",
      "Labels shape: (200000, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# File import\n",
    "# Sample filenames are:\n",
    "# CeBr10kSingle_1.txt -> single events, \n",
    "# CeBr10kSingle_2.txt -> single events\n",
    "# CeBr10k_1.txt -> mixed single and double events \n",
    "# CeBr10.txt -> small file of 10 samples\n",
    "# CeBr2Mil_Mix.txt -> 2 million mixed samples of simulated events\n",
    "\n",
    "# Flag import, since we can now import 200k events from .npy files\n",
    "from_file = False\n",
    "if from_file:\n",
    "\n",
    "    folder = \"simulated\"\n",
    "    filename = \"CeBr2Mil_Mix.txt\"\n",
    "    num_samples = 2e5\n",
    "    #folder = \"sample\"\n",
    "    #filename = \"CeBr10k_1.txt\"\n",
    "    #num_samples = 1e3\n",
    "\n",
    "    data = import_data(folder=folder, filename=filename, num_samples=num_samples)\n",
    "    images = data[filename][\"images\"]\n",
    "    energies = data[filename][\"energies\"]\n",
    "    positions = data[filename][\"positions\"]\n",
    "    labels = to_categorical(data[filename][\"labels\"])\n",
    "    n_classes = labels.shape[1]\n",
    "else:\n",
    "    images = load_feature_representation(\"images_200k.npy\")\n",
    "    energies = load_feature_representation(\"energies_200k.npy\")\n",
    "    positions = load_feature_representation(\"positions_200k.npy\")\n",
    "    labels = load_feature_representation(\"labels_200k.npy\")\n",
    "\n",
    "n_classes = labels.shape[1]\n",
    "print(\"Number of classes: {}\".format(n_classes))\n",
    "print(\"Images shape: {}\".format(images.shape))\n",
    "print(\"Energies shape: {}\".format(energies.shape))\n",
    "print(\"Positions shape: {}\".format(positions.shape))\n",
    "print(\"Labels shape: {}\".format(labels.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image data shape: (200000, 16, 16, 3)\n"
     ]
    }
   ],
   "source": [
    "# VGG16 expects 3 channels. Solving this by concatenating the image data \n",
    "# to itself, to form three identical channels\n",
    "\n",
    "images = np.concatenate((images, images, images), axis=3)\n",
    "print(\"Image data shape: {}\".format(images.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with custom dense network\n",
    "### Multiple dense model\n",
    "Build a dense model for each pretrained model form which a feature representation has been saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keys: model names, Values: depth to compare at.\n",
    "pretrained_models = {\n",
    "    \"DenseNet121\":None, #8\n",
    "    \"DenseNet169\":None, #8\n",
    "    \"DenseNet201\":None, #8\n",
    "    \"InceptionResNetV2\":None, #8\n",
    "    \"InceptionV3\":None, #8\n",
    "    \"MobileNet\":None, #8\n",
    "    \"MobileNetV2\":None, #5\n",
    "    \"NASNetLarge\":None, #4\n",
    "    \"NASNetMobile\":None, #4\n",
    "    \"ResNet50\":None, #8\n",
    "    \"VGG16\":None,\n",
    "    \"VGG19\":None,\n",
    "    \"Xception\":None, #6\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train a fully-connected network to classify based on\n",
    "# extracted features\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "# Set up and train a network for each of the pretrained features available    \n",
    "for net, depth in pretrained_models.items():\n",
    "    print(\"Running for:\", net)\n",
    "    \n",
    "    # Load features\n",
    "    if depth is None:\n",
    "        depth = \"full\"\n",
    "    features_filename = net + \"_d\" + str(depth) + \"_\" + str(images.shape[0]) + \".npy\"\n",
    "    pretrained_features = load_feature_representation(features_filename)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=pretrained_features.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Remove nan values from pretrained_features (and remove labels which produces them)\n",
    "    nan_indices = []\n",
    "    for i in range(pretrained_features.shape[0]):\n",
    "        if np.isnan(pretrained_features[i,:]).any():\n",
    "            nan_indices.append(i)\n",
    "    pretrained_features = np.delete(pretrained_features, nan_indices, axis=0)\n",
    "    tmp_labels = np.delete(labels, nan_indices, axis=0)\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(pretrained_features, tmp_labels, test_size = 0.2)    \n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x_train, \n",
    "        y_train, \n",
    "        epochs=10, \n",
    "        batch_size=32,\n",
    "        validation_data=(x_test, y_test))\n",
    "    \n",
    "    acc_filename = net + \"_d\" + str(depth) + \"_\" + str(pretrained_features.shape[0]) + \"_accuracy\"\n",
    "    loss_filename = net + \"_d\" + str(depth) + \"_\" + str(pretrained_features.shape[0]) + \"_loss\"\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.savefig(acc_filename)\n",
    "    plt.clf()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.savefig(loss_filename)\n",
    "    plt.clf()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single dense model for one pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143998 samples, validate on 16000 samples\n",
      "Epoch 1/5\n",
      "143998/143998 [==============================] - 22s 152us/sample - loss: 0.3227 - accuracy: 0.8628 - val_loss: 0.3557 - val_accuracy: 0.8619\n",
      "Epoch 2/5\n",
      "143998/143998 [==============================] - 22s 152us/sample - loss: 0.2606 - accuracy: 0.8987 - val_loss: 0.2368 - val_accuracy: 0.9099\n",
      "Epoch 3/5\n",
      "143998/143998 [==============================] - 23s 161us/sample - loss: 0.2293 - accuracy: 0.9139 - val_loss: 0.2090 - val_accuracy: 0.9243\n",
      "Epoch 4/5\n",
      "143998/143998 [==============================] - 22s 155us/sample - loss: 0.2174 - accuracy: 0.9204 - val_loss: 0.2175 - val_accuracy: 0.9295\n",
      "Epoch 5/5\n",
      "143998/143998 [==============================] - 22s 151us/sample - loss: 0.2084 - accuracy: 0.9249 - val_loss: 0.1979 - val_accuracy: 0.9294\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Single net testing\n",
    "\n",
    "net = \"DenseNet201\"\n",
    "depth = \"full\"\n",
    "epochs = 5\n",
    "# Load features\n",
    "features_filename = net + \"_d\" + str(depth) + \"_\" + str(images.shape[0]) + \".npy\"\n",
    "pretrained_features = load_feature_representation(features_filename)\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=pretrained_features.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Remove nan values from pretrained_features (and remove labels which produces them)\n",
    "nan_indices = []\n",
    "for i in range(pretrained_features.shape[0]):\n",
    "    if np.isnan(pretrained_features[i,:]).any():\n",
    "        nan_indices.append(i)\n",
    "pretrained_features = np.delete(pretrained_features, nan_indices, axis=0)\n",
    "tmp_labels = np.delete(labels, nan_indices, axis=0)\n",
    "tmp_positions = np.delete(positions, nan_indices, axis=0)\n",
    "tmp_energies = np.delete(energies, nan_indices, axis=0)\n",
    "\n",
    "labels_positions_energies = np.concatenate((tmp_labels, tmp_positions, tmp_energies), axis=1)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(pretrained_features, labels_positions_energies, test_size = 0.2)    \n",
    "\n",
    "test_positions = y_test[:, 2:6]\n",
    "train_positions = y_train[:, 2:6]\n",
    "test_energies = y_test[:, 6:]\n",
    "train_energies = y_train[:, 6:]\n",
    "y_test = y_test[:, :2]\n",
    "y_train = y_train[:, :2]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=epochs, \n",
    "    batch_size=32,\n",
    "    validation_split=0.1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test set and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_predicted = model.predict(x_test)\n",
    "\n",
    "# indices, relative distances and relative energies for test set\n",
    "single_indices, double_indices, close_indices = event_indices(test_positions)\n",
    "rel_distance = relative_distance(test_positions)\n",
    "rel_energy = relative_energy(test_energies)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean dist all double events:  7.635565034979257\n",
      "Mean dist correct double events:  10.396661758942615\n",
      "Mean dist wrong double events:  6.782346695243955\n",
      "Ratio of correctly classified double events:  0.23606639839034205\n",
      "Ratio correctly classified events with dist < 3mm:  0.000462962962962963\n"
     ]
    }
   ],
   "source": [
    "# Separate correct and wrong classifications\n",
    "\n",
    "correct_doubles = np.where(tmp_predicted[double_indices][:,1] == 1)[0]\n",
    "wrong_doubles = np.where(tmp_predicted[double_indices][:,1] != 1)[0]\n",
    "\n",
    "mean_correct = np.mean(rel_distance[double_indices][correct_doubles])\n",
    "mean_wrong = np.mean(rel_distance[double_indices][wrong_doubles])\n",
    "mean_all = np.mean(rel_distance[double_indices])\n",
    "\n",
    "ratio_doubles = len(correct_doubles) / len(double_indices)\n",
    "\n",
    "# Ratio of correctly classified double events with a distance between\n",
    "# events < 3mm\n",
    "n_close = len(close_indices)\n",
    "n_close_correct = len(np.where(rel_distance[double_indices][correct_doubles] < 3.0)[0])\n",
    "ratio_close = n_close_correct / n_close\n",
    "\n",
    "# Output\n",
    "print(\"Mean dist all double events: \", mean_all)\n",
    "print(\"Mean dist correct double events: \", mean_correct)\n",
    "print(\"Mean dist wrong double events: \", mean_wrong)\n",
    "print(\"Ratio of correctly classified double events: \", ratio_doubles)\n",
    "print(\"Ratio correctly classified events with dist < 3mm: \", ratio_close)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Numbers')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histograms\n",
    "fig, ax = plt.subplots(1, 2, sharex='col', sharey='row', figsize=(12,4))\n",
    "ax[0].hist(rel_distance[double_indices][correct_doubles], bins=50, alpha=0.5, label=\"correct\")\n",
    "ax[0].hist(rel_distance[double_indices][wrong_doubles], bins=50, alpha=0.5, label=\"wrong\")\n",
    "ax[0].set_title(\"Distances between classified events\")\n",
    "ax[0].legend()\n",
    "ax[1].text(0, 600, r'  $\\mu$ correct = {:.2f} mm'.format(mean_correct))\n",
    "ax[1].text(0, 550, r'  $\\mu$ wrong = {:.2f} mm'.format(mean_wrong))\n",
    "ax[1].text(0, 500, r'  ratio doubles < 3mm = {:.2f}'.format(ratio_doubles))\n",
    "ax[1].text(0, 650, r'  $\\mu$ all = {:.2f} mm'.format(mean_all))\n",
    "ax[1].set_title(\"Numbers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for: DenseNet121\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/5\n",
      "144000/144000 [==============================] - 22s 150us/sample - loss: 0.3484 - accuracy: 0.8488 - val_loss: 0.2748 - val_accuracy: 0.8937\n",
      "Epoch 2/5\n",
      "144000/144000 [==============================] - 21s 146us/sample - loss: 0.2782 - accuracy: 0.8901 - val_loss: 0.2663 - val_accuracy: 0.8966\n",
      "Epoch 3/5\n",
      "144000/144000 [==============================] - 21s 146us/sample - loss: 0.2552 - accuracy: 0.9023 - val_loss: 0.2655 - val_accuracy: 0.8941\n",
      "Epoch 4/5\n",
      "144000/144000 [==============================] - 20s 141us/sample - loss: 0.2390 - accuracy: 0.9108 - val_loss: 0.2383 - val_accuracy: 0.9154\n",
      "Epoch 5/5\n",
      "144000/144000 [==============================] - 19s 135us/sample - loss: 0.2314 - accuracy: 0.9147 - val_loss: 0.2271 - val_accuracy: 0.9179\n",
      "Running for: DenseNet169\n",
      "Train on 143998 samples, validate on 16000 samples\n",
      "Epoch 1/5\n",
      "143998/143998 [==============================] - 20s 138us/sample - loss: 0.3241 - accuracy: 0.8631 - val_loss: 0.2922 - val_accuracy: 0.8799\n",
      "Epoch 2/5\n",
      "143998/143998 [==============================] - 20s 137us/sample - loss: 0.2669 - accuracy: 0.8967 - val_loss: 0.2616 - val_accuracy: 0.8991\n",
      "Epoch 3/5\n",
      "143998/143998 [==============================] - 20s 137us/sample - loss: 0.2449 - accuracy: 0.9077 - val_loss: 0.2303 - val_accuracy: 0.9226\n",
      "Epoch 4/5\n",
      "143998/143998 [==============================] - 19s 134us/sample - loss: 0.2325 - accuracy: 0.9139 - val_loss: 0.2255 - val_accuracy: 0.9184\n",
      "Epoch 5/5\n",
      "143998/143998 [==============================] - 20s 137us/sample - loss: 0.2239 - accuracy: 0.9180 - val_loss: 0.2571 - val_accuracy: 0.9009\n",
      "Running for: DenseNet201\n",
      "Train on 143998 samples, validate on 16000 samples\n",
      "Epoch 1/5\n",
      "143998/143998 [==============================] - 19s 135us/sample - loss: 0.3256 - accuracy: 0.8619 - val_loss: 0.3170 - val_accuracy: 0.8594\n",
      "Epoch 2/5\n",
      "143998/143998 [==============================] - 19s 135us/sample - loss: 0.2618 - accuracy: 0.8981 - val_loss: 0.2168 - val_accuracy: 0.9195\n",
      "Epoch 3/5\n",
      "143998/143998 [==============================] - 19s 135us/sample - loss: 0.2321 - accuracy: 0.9131 - val_loss: 0.1959 - val_accuracy: 0.9303\n",
      "Epoch 4/5\n",
      "143998/143998 [==============================] - 19s 134us/sample - loss: 0.2179 - accuracy: 0.9208 - val_loss: 0.2373 - val_accuracy: 0.9137\n",
      "Epoch 5/5\n",
      "143998/143998 [==============================] - 19s 134us/sample - loss: 0.2091 - accuracy: 0.9250 - val_loss: 0.2699 - val_accuracy: 0.8876\n",
      "Running for: InceptionResNetV2\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/5\n",
      "144000/144000 [==============================] - 19s 133us/sample - loss: 0.4825 - accuracy: 0.7609 - val_loss: 0.4533 - val_accuracy: 0.7679\n",
      "Epoch 2/5\n",
      "144000/144000 [==============================] - 19s 133us/sample - loss: 0.3887 - accuracy: 0.8296 - val_loss: 0.3291 - val_accuracy: 0.8677\n",
      "Epoch 3/5\n",
      "144000/144000 [==============================] - 19s 132us/sample - loss: 0.3546 - accuracy: 0.8498 - val_loss: 0.3494 - val_accuracy: 0.8444\n",
      "Epoch 4/5\n",
      "144000/144000 [==============================] - 19s 132us/sample - loss: 0.3365 - accuracy: 0.8595 - val_loss: 0.3040 - val_accuracy: 0.8783\n",
      "Epoch 5/5\n",
      "144000/144000 [==============================] - 19s 131us/sample - loss: 0.3251 - accuracy: 0.8671 - val_loss: 0.3269 - val_accuracy: 0.8612\n",
      "Running for: InceptionV3\n",
      "Train on 144000 samples, validate on 16000 samples\n",
      "Epoch 1/5\n",
      "144000/144000 [==============================] - 18s 128us/sample - loss: 0.4710 - accuracy: 0.7672 - val_loss: 0.4097 - val_accuracy: 0.8099\n",
      "Epoch 2/5\n",
      "144000/144000 [==============================] - 18s 124us/sample - loss: 0.3814 - accuracy: 0.8306 - val_loss: 0.3535 - val_accuracy: 0.8519\n",
      "Epoch 3/5\n",
      "144000/144000 [==============================] - 18s 127us/sample - loss: 0.3484 - accuracy: 0.8513 - val_loss: 0.3184 - val_accuracy: 0.8701\n",
      "Epoch 4/5\n",
      "144000/144000 [==============================] - 18s 126us/sample - loss: 0.3282 - accuracy: 0.8629 - val_loss: 0.3314 - val_accuracy: 0.8637\n",
      "Epoch 5/5\n",
      "144000/144000 [==============================] - 18s 126us/sample - loss: 0.3153 - accuracy: 0.8717 - val_loss: 0.3142 - val_accuracy: 0.8792\n",
      "Running for: MobileNet\n",
      "Train on 143998 samples, validate on 16000 samples\n",
      "Epoch 1/5\n",
      "143998/143998 [==============================] - 31s 215us/sample - loss: 7.6791 - accuracy: 0.5006 - val_loss: 7.7597 - val_accuracy: 0.4954\n",
      "Epoch 2/5\n",
      "143998/143998 [==============================] - 31s 214us/sample - loss: 7.6806 - accuracy: 0.5006 - val_loss: 7.7597 - val_accuracy: 0.4954\n",
      "Epoch 3/5\n",
      "143998/143998 [==============================] - 31s 214us/sample - loss: 7.6806 - accuracy: 0.5006 - val_loss: 7.7597 - val_accuracy: 0.4954\n",
      "Epoch 4/5\n",
      "143998/143998 [==============================] - 31s 214us/sample - loss: 7.6806 - accuracy: 0.5006 - val_loss: 7.7597 - val_accuracy: 0.4954\n",
      "Epoch 5/5\n",
      "143998/143998 [==============================] - 31s 214us/sample - loss: 7.6806 - accuracy: 0.5006 - val_loss: 7.7597 - val_accuracy: 0.4954\n",
      "Running for: MobileNetV2\n",
      "Train on 143998 samples, validate on 16000 samples\n",
      "Epoch 1/5\n",
      "143998/143998 [==============================] - 19s 135us/sample - loss: 7.6960 - accuracy: 0.4993 - val_loss: 7.7049 - val_accuracy: 0.4990\n",
      "Epoch 2/5\n",
      "143998/143998 [==============================] - 19s 133us/sample - loss: 7.6984 - accuracy: 0.4994 - val_loss: 7.7049 - val_accuracy: 0.4990\n",
      "Epoch 3/5\n",
      "143998/143998 [==============================] - 19s 134us/sample - loss: 7.6984 - accuracy: 0.4994 - val_loss: 7.7049 - val_accuracy: 0.4990\n",
      "Epoch 4/5\n",
      "143998/143998 [==============================] - 19s 133us/sample - loss: 7.6984 - accuracy: 0.4994 - val_loss: 7.7049 - val_accuracy: 0.4990\n",
      "Epoch 5/5\n",
      "143998/143998 [==============================] - 19s 133us/sample - loss: 7.6984 - accuracy: 0.4994 - val_loss: 7.7049 - val_accuracy: 0.4990\n",
      "Running for: NASNetLarge\n",
      "Train on 143998 samples, validate on 16000 samples\n",
      "Epoch 1/5\n",
      "143998/143998 [==============================] - 31s 212us/sample - loss: 0.3600 - accuracy: 0.8469 - val_loss: 0.2926 - val_accuracy: 0.8874\n",
      "Epoch 2/5\n",
      "143998/143998 [==============================] - 30s 209us/sample - loss: 0.2797 - accuracy: 0.8926 - val_loss: 0.2820 - val_accuracy: 0.8920\n",
      "Epoch 3/5\n",
      "143998/143998 [==============================] - 30s 209us/sample - loss: 0.2540 - accuracy: 0.9052 - val_loss: 0.2555 - val_accuracy: 0.9050\n",
      "Epoch 4/5\n",
      "143998/143998 [==============================] - 30s 209us/sample - loss: 0.2372 - accuracy: 0.9130 - val_loss: 0.2466 - val_accuracy: 0.9094\n",
      "Epoch 5/5\n",
      "143998/143998 [==============================] - 30s 209us/sample - loss: 0.2262 - accuracy: 0.9180 - val_loss: 0.2466 - val_accuracy: 0.9106\n",
      "Running for: NASNetMobile\n",
      "Train on 143998 samples, validate on 16000 samples\n",
      "Epoch 1/5\n",
      "143998/143998 [==============================] - 19s 135us/sample - loss: 0.3782 - accuracy: 0.8374 - val_loss: 0.3443 - val_accuracy: 0.8636\n",
      "Epoch 2/5\n",
      "143998/143998 [==============================] - 19s 134us/sample - loss: 0.2953 - accuracy: 0.8847 - val_loss: 0.2802 - val_accuracy: 0.8939\n",
      "Epoch 3/5\n",
      "143998/143998 [==============================] - 19s 133us/sample - loss: 0.2642 - accuracy: 0.9001 - val_loss: 0.2572 - val_accuracy: 0.9076\n",
      "Epoch 4/5\n",
      "143998/143998 [==============================] - 19s 135us/sample - loss: 0.2459 - accuracy: 0.9092 - val_loss: 0.2601 - val_accuracy: 0.9086\n",
      "Epoch 5/5\n",
      "143998/143998 [==============================] - 19s 134us/sample - loss: 0.2350 - accuracy: 0.9138 - val_loss: 0.2407 - val_accuracy: 0.9143\n",
      "Running for: ResNet50\n",
      "Train on 143998 samples, validate on 16000 samples\n",
      "Epoch 1/5\n",
      "143998/143998 [==============================] - 51s 357us/sample - loss: 0.6962 - accuracy: 0.4989 - val_loss: 0.6933 - val_accuracy: 0.4923\n",
      "Epoch 2/5\n",
      "143998/143998 [==============================] - 51s 358us/sample - loss: 0.6932 - accuracy: 0.5009 - val_loss: 0.6932 - val_accuracy: 0.4923\n",
      "Epoch 3/5\n",
      "143998/143998 [==============================] - 51s 354us/sample - loss: 0.6932 - accuracy: 0.4989 - val_loss: 0.6933 - val_accuracy: 0.4923\n",
      "Epoch 4/5\n",
      "143998/143998 [==============================] - 51s 356us/sample - loss: 0.6932 - accuracy: 0.4989 - val_loss: 0.6933 - val_accuracy: 0.4923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "143998/143998 [==============================] - 51s 354us/sample - loss: 0.6932 - accuracy: 0.4986 - val_loss: 0.6931 - val_accuracy: 0.5077\n",
      "Running for: VGG16\n",
      "Train on 143998 samples, validate on 16000 samples\n",
      "Epoch 1/5\n",
      "143998/143998 [==============================] - 19s 134us/sample - loss: 0.3112 - accuracy: 0.8736 - val_loss: 0.2548 - val_accuracy: 0.9058\n",
      "Epoch 2/5\n",
      "143998/143998 [==============================] - 19s 132us/sample - loss: 0.2723 - accuracy: 0.8959 - val_loss: 0.2610 - val_accuracy: 0.8992\n",
      "Epoch 3/5\n",
      "143998/143998 [==============================] - 19s 132us/sample - loss: 0.2630 - accuracy: 0.8998 - val_loss: 0.2603 - val_accuracy: 0.8991\n",
      "Epoch 4/5\n",
      "143998/143998 [==============================] - 19s 132us/sample - loss: 0.2590 - accuracy: 0.9023 - val_loss: 0.2609 - val_accuracy: 0.8982\n",
      "Epoch 5/5\n",
      "143998/143998 [==============================] - 19s 132us/sample - loss: 0.2531 - accuracy: 0.9052 - val_loss: 0.2485 - val_accuracy: 0.9066\n",
      "Running for: VGG19\n",
      "Train on 143998 samples, validate on 16000 samples\n",
      "Epoch 1/5\n",
      "143998/143998 [==============================] - 20s 139us/sample - loss: 0.3759 - accuracy: 0.8396 - val_loss: 0.3823 - val_accuracy: 0.8253\n",
      "Epoch 2/5\n",
      "143998/143998 [==============================] - 20s 137us/sample - loss: 0.3271 - accuracy: 0.8682 - val_loss: 0.3129 - val_accuracy: 0.8823\n",
      "Epoch 3/5\n",
      "143998/143998 [==============================] - 20s 137us/sample - loss: 0.3134 - accuracy: 0.8756 - val_loss: 0.3127 - val_accuracy: 0.8766\n",
      "Epoch 4/5\n",
      "143998/143998 [==============================] - 20s 136us/sample - loss: 0.3053 - accuracy: 0.8801 - val_loss: 0.3187 - val_accuracy: 0.8706\n",
      "Epoch 5/5\n",
      "143998/143998 [==============================] - 20s 136us/sample - loss: 0.2951 - accuracy: 0.8853 - val_loss: 0.2732 - val_accuracy: 0.8962\n",
      "Running for: Xception\n",
      "Train on 143998 samples, validate on 16000 samples\n",
      "Epoch 1/5\n",
      "143998/143998 [==============================] - 40s 280us/sample - loss: 7.6697 - accuracy: 0.5012 - val_loss: 7.8270 - val_accuracy: 0.4911\n",
      "Epoch 2/5\n",
      "143998/143998 [==============================] - 40s 277us/sample - loss: 7.6707 - accuracy: 0.5012 - val_loss: 7.8270 - val_accuracy: 0.4911\n",
      "Epoch 3/5\n",
      "143998/143998 [==============================] - 40s 278us/sample - loss: 7.6707 - accuracy: 0.5012 - val_loss: 7.8270 - val_accuracy: 0.4911\n",
      "Epoch 4/5\n",
      "143998/143998 [==============================] - 40s 280us/sample - loss: 7.6707 - accuracy: 0.5012 - val_loss: 7.8270 - val_accuracy: 0.4911\n",
      "Epoch 5/5\n",
      "143998/143998 [==============================] - 40s 280us/sample - loss: 7.6707 - accuracy: 0.5012 - val_loss: 7.8270 - val_accuracy: 0.4911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 864x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up and train a network for each of the pretrained features available \n",
    "# and output histograms of correct vs incorrect classifications\n",
    "for net, depth in pretrained_models.items():\n",
    "    print(\"Running for:\", net)\n",
    "    \n",
    "    # Load features\n",
    "    if depth is None:\n",
    "        depth = \"full\"\n",
    "    features_filename = net + \"_d\" + str(depth) + \"_\" + str(images.shape[0]) + \".npy\"\n",
    "    pretrained_features = load_feature_representation(features_filename)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=pretrained_features.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Remove nan values from pretrained_features (and remove labels which produces them)\n",
    "    nan_indices = []\n",
    "    for i in range(pretrained_features.shape[0]):\n",
    "        if np.isnan(pretrained_features[i,:]).any():\n",
    "            nan_indices.append(i)\n",
    "    pretrained_features = np.delete(pretrained_features, nan_indices, axis=0)\n",
    "    tmp_labels = np.delete(labels, nan_indices, axis=0)\n",
    "    tmp_positions = np.delete(positions, nan_indices, axis=0)\n",
    "\n",
    "    labels_positions = np.concatenate((tmp_labels, tmp_positions), axis=1)\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(pretrained_features, labels_positions, test_size = 0.2)    \n",
    "\n",
    "    test_positions = y_test[:, 2:]\n",
    "    train_positions = y_train[:, 2:]\n",
    "    y_test = y_test[:, :2]\n",
    "    y_train = y_train[:, :2]  \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x_train, \n",
    "        y_train, \n",
    "        epochs=5, \n",
    "        batch_size=32,\n",
    "        validation_split=0.1)\n",
    "    \n",
    "    tmp_predicted = model.predict(x_test)\n",
    "    tmp_results = tmp_predicted.argmax(axis=-1)\n",
    "    double_events = double_event_distance(tmp_results, test_positions)\n",
    "    \n",
    "    # double_events contain (predicted_class, distance between event origins)\n",
    "    # All the events are double events, so if predicted class is 0, the predicted\n",
    "    # class is wrong.\n",
    "\n",
    "    correct = np.where(double_events[:,0] == 1)\n",
    "    wrong = np.where(double_events[:,0] == 0)\n",
    "\n",
    "    mean_correct = np.mean(double_events[correct][:,1])\n",
    "    mean_wrong = np.mean(double_events[wrong][:,1])\n",
    "    mean_all = np.mean(double_events[:,1])\n",
    "\n",
    "    # Ratio of correctly classified double events with a distance between\n",
    "    # events < 3mm\n",
    "    close_events = np.where(double_events[:,1] < 3.0)\n",
    "    ratio_close = np.count_nonzero(double_events[close_events][:,0])/len(double_events[close_events][:,0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    del(tmp_predicted)\n",
    "    del(tmp_results)\n",
    "    # Output\n",
    "\n",
    "    # Histograms\n",
    "    fig, ax = plt.subplots(1, 2, sharex='col', sharey='row', figsize=(12,4))\n",
    "    ax[0].hist(double_events[:,1], bins=50)\n",
    "    ax[0].set_title(\"Distances between all double events\")\n",
    "    ax[0].text(0, 880, r'$\\mu$ = {:.2f} mm'.format(mean_all))\n",
    "    ax[1].hist(double_events[correct][:,1], bins=50, alpha=0.5, label=\"correct\")\n",
    "    ax[1].hist(double_events[wrong][:,1], bins=50, alpha=0.5, label=\"wrong\")\n",
    "    ax[1].set_title(\"Distances between classified double events\")\n",
    "    ax[1].legend()\n",
    "    ax[1].text(0, 880, r'$\\mu$ correct = {:.2f} mm'.format(mean_correct))\n",
    "    ax[1].text(0, 820, r'$\\mu$ wrong = {:.2f} mm'.format(mean_wrong))\n",
    "    ax[1].text(0, 760, r'ratio correct < 3mm = {:.2f}'.format(ratio_close))\n",
    "    figure_name = net + \"_d\" + str(depth) + \"_\" + str(images.shape[0]) + \"_distance.png\"\n",
    "    fig.savefig(figure_name, format=\"png\")\n",
    "    fig.clf()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pretrained_venv",
   "language": "python",
   "name": "pretrained_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
