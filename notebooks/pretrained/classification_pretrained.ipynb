{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using pretrained, well-known models\n",
    "This notebook aims to create a set of benchmarks for the project, using well-known, thoroughly studied models.\n",
    "Either with pretrained weights, or training with new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "### 21.08.19\n",
    "Attempting to classify with VGG has not proven effective yet.\n",
    "Initially, the image data was note scaled at all. Implemented scaling in the\n",
    "import function, using min-max scaling of the value. This preserves the inherent\n",
    "intensity difference between images.\n",
    "\n",
    "The number of layers of VGG16 used is varied between 3 to 9 without noticable\n",
    "difference. Attempting to find out why, by analyzing the extracted features.\n",
    "The idea is that in order to classify, the feature distribution should be\n",
    "different for images containing single and double events.\n",
    "I first attempt this with manual qualitative inspection.\n",
    "\n",
    "Manual, qualitative inspection reveals that the distributions look very similar.\n",
    "Performing a quantitative study using Kolmogorov-Smirnov two-sample test,\n",
    "comparing the distribution for each feature.\n",
    "\n",
    "* For 1 block (depth 3), the pvalue returned from comparisons is 1.0 for all features.\n",
    "* For 2 blocks (depth 6), the pvalue returned from comparisons is 1.0 for all features.\n",
    "* For 3 blocks (depth 10), the pvalue returned from comparisons is 1.0 for all features.\n",
    "* For 4 blocks, (depth 14) the pvalue returned from comparisons is 1.0 for all features.\n",
    "* For 5 blocks, (depth 18) the pvalue returned from comparisons is 1.0 for all features.\n",
    "\n",
    "This indicates that extracting features using vgg16 doesn't work for classification.\n",
    "I still want to confirm that the weights of the vgg layers are the imagenet weights.\n",
    "\n",
    "### 22.08.19\n",
    "Going to use a reference image from imagenet to verify that the vgg-layers behave\n",
    "as expected.\n",
    "* Reference produces very similar feature output as simulated data\n",
    "\n",
    "Rewrote the vgg_model script to be able to import any pretrained model from\n",
    "tensorflow, and extended data import to handle single files (for large file)\n",
    "and possibility to specify number of samples to include.\n",
    "\n",
    "### 23.08.19\n",
    "Import scripts fixed so that array dimensions are correct independent of folder or single\n",
    "file import.\n",
    "\n",
    "Tests on multiple nets with 10k events give same results as for VGG.\n",
    "\n",
    "### 24.08.19\n",
    "Running checks on feature distribution with the full networks and 200k events.\n",
    "With 200k events there are models which from the p-value given by the KS two-sample test\n",
    "should be possible to classify with. Interstingly, VGG16 and VGG19 show a large variance in which\n",
    "features are seemingly drawn from different distributions.\n",
    "\n",
    "Need to run for: NASNetLarge, ResNet50 and Xception, but running into memory problems.\n",
    "\n",
    "### 25.08.19\n",
    "Some NaN values in output features from VGG. When removed, the network trains to acc = 0.9\n",
    "on the features. \n",
    "\n",
    "Implemented save_feature_representation and load_feature_representation.\n",
    "\n",
    "### 04.09.19\n",
    "Storing 200k events to use for generating feature representations for classification.\n",
    "\n",
    "### 09.09.19\n",
    "Previously trained fully connected nets on the feature output from all pretrained networks available\n",
    "at full depth. The fully-connected nets were configured so that the input layer had a number of nodes\n",
    "equal to the number of features output from the convolutional block before it (because the input shape\n",
    "must be defined). After that, two layers of 512 nodes with RELu actication functions, into one\n",
    "2-node layer with softmax.\n",
    "Results:\n",
    "* __DenseNet121__: \n",
    "    Accuracy starts around 0.86 first epoch and rises to just under 0.94 at maximum.\n",
    "    Somewhat unstable.\n",
    "* __DenseNet169__: \n",
    "    Very similar to DenseNet121, but starts at around 0.90\n",
    "* __DenseNet201__: \n",
    "    Very similar to other two DenseNets, but more stable above 0.92 for epoch 4+.\n",
    "* __InceptionResNetV2__: \n",
    "    Starts at 0.84, maxes just above 0.88. A bit zig-zag.\n",
    "* __InceptionV3__: \n",
    "    Similar to V2 above, but starts lower and maxes out higher (0.90). Huge dip at epoch 7,\n",
    "    around same spot as DenseNets.\n",
    "* __MobileNet__: \n",
    "    Didn't learn anything.\n",
    "* __MobileNetV2__: \n",
    "    Didn't learn anything.\n",
    "* __NASNetLarge__: \n",
    "    Starts at 0.88, maxes out at 0.92 ish. Stable nice increase in accuracy with small dip at last epoch.\n",
    "* __NASNetMobile__: \n",
    "    Roughly same as NASNetLarge, but a little more zig-zag in the curve.\n",
    "* __ResNet50__: \n",
    "    Didn't learn anything.\n",
    "* __VGG16__: \n",
    "    A lot of zig-zag, but between 0.88 and just under 0.92 (max), so it's a small span.\n",
    "    Stops zig-zagging on max and becomes pretty much flat.\n",
    "* __VGG19__: \n",
    "    Smaller zig-zag distances, steady increase up to around 0.90 for maximum.\n",
    "* __Xception__: \n",
    "    Starts high (0.88) and slowly rises to a maximum of 0.92, a dip around epoch 6 like many others.\n",
    "    \n",
    "From this I think the nets that didn't weren't able to extract anything from the features can be\n",
    "ignored for a while, to focus on those that performed well. This leaves the following nets as possible\n",
    "points of interest:\n",
    "* DenseNet201 (best of the DenseNets, keep the others in mind)\n",
    "* NASNetLarge (keep NASNetMobile in mind)\n",
    "* VGG16\n",
    "* VGG19\n",
    "* Xception\n",
    "\n",
    "### 15.09.19\n",
    "Simple implementation of distance-checking of double events done.\n",
    "Results indicate that the model struggles more with events that are close together.\n",
    "Specifically, I've looked at the ratio of correctly classified double events where\n",
    "the events are less than 3mm apart.\n",
    "\n",
    "### 16.09.19\n",
    "* Compare feature distributions of single events and double events with distance < 3mm.\n",
    "Plots produced. There is a similar difference between single events and close double events,\n",
    "as between single events and double events in general.\n",
    "\n",
    "* Is the number of close events large enough to be confident in the result of the KS two-sample test?\n",
    "* What does the above result imply?\n",
    "* Is weighting the training data such that close events are weighted higher than others a good idea? Would this introduce a bias?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "Things to get done before further exploration:\n",
    "* Implement functions to save and load trained fully-connected networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from master_data_functions.functions import import_data,save_feature_representation,load_feature_representation\n",
    "from master_models.pretrained import pretrained_model\n",
    "from master_data_functions.functions import double_event_distance, get_close_events\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 2\n",
      "Images shape: (200000, 16, 16, 1)\n",
      "Energies shape: (200000, 2)\n",
      "Positions shape: (200000, 4)\n",
      "Labels shape: (200000, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# File import\n",
    "# Sample filenames are:\n",
    "# CeBr10kSingle_1.txt -> single events, \n",
    "# CeBr10kSingle_2.txt -> single events\n",
    "# CeBr10k_1.txt -> mixed single and double events \n",
    "# CeBr10.txt -> small file of 10 samples\n",
    "# CeBr2Mil_Mix.txt -> 2 million mixed samples of simulated events\n",
    "\n",
    "# Flag import, since we can now import 200k events from .npy files\n",
    "from_file = False\n",
    "if from_file:\n",
    "\n",
    "    folder = \"simulated\"\n",
    "    filename = \"CeBr2Mil_Mix.txt\"\n",
    "    num_samples = 2e5\n",
    "    #folder = \"sample\"\n",
    "    #filename = \"CeBr10k_1.txt\"\n",
    "    #num_samples = 1e3\n",
    "\n",
    "    data = import_data(folder=folder, filename=filename, num_samples=num_samples)\n",
    "    images = data[filename][\"images\"]\n",
    "    energies = data[filename][\"energies\"]\n",
    "    positions = data[filename][\"positions\"]\n",
    "    labels = to_categorical(data[filename][\"labels\"])\n",
    "    n_classes = labels.shape[1]\n",
    "else:\n",
    "    images = load_feature_representation(\"images_200k.npy\")\n",
    "    energies = load_feature_representation(\"energies_200k.npy\")\n",
    "    positions = load_feature_representation(\"positions_200k.npy\")\n",
    "    labels = load_feature_representation(\"labels_200k.npy\")\n",
    "\n",
    "n_classes = labels.shape[1]\n",
    "print(\"Number of classes: {}\".format(n_classes))\n",
    "print(\"Images shape: {}\".format(images.shape))\n",
    "print(\"Energies shape: {}\".format(energies.shape))\n",
    "print(\"Positions shape: {}\".format(positions.shape))\n",
    "print(\"Labels shape: {}\".format(labels.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image data shape: (200000, 16, 16, 3)\n"
     ]
    }
   ],
   "source": [
    "# VGG16 expects 3 channels. Solving this by concatenating the image data \n",
    "# to itself, to form three identical channels\n",
    "\n",
    "images = np.concatenate((images, images, images), axis=3)\n",
    "print(\"Image data shape: {}\".format(images.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with custom dense network\n",
    "### Multiple dense model\n",
    "Build a dense model for each pretrained model form which a feature representation has been saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train a fully-connected network to classify based on\n",
    "# extracted features\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "# Set up and train a network for each of the pretrained features available    \n",
    "for net, depth in pretrained_models.items():\n",
    "    print(\"Running for:\", net)\n",
    "    \n",
    "    # Load features\n",
    "    if depth is None:\n",
    "        depth = \"full\"\n",
    "    features_filename = net + \"_d\" + str(depth) + \"_\" + str(images.shape[0]) + \".npy\"\n",
    "    pretrained_features = load_feature_representation(features_filename)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=pretrained_features.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Remove nan values from pretrained_features (and remove labels which produces them)\n",
    "    nan_indices = []\n",
    "    for i in range(pretrained_features.shape[0]):\n",
    "        if np.isnan(pretrained_features[i,:]).any():\n",
    "            nan_indices.append(i)\n",
    "    pretrained_features = np.delete(pretrained_features, nan_indices, axis=0)\n",
    "    tmp_labels = np.delete(labels, nan_indices, axis=0)\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(pretrained_features, tmp_labels, test_size = 0.2)    \n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x_train, \n",
    "        y_train, \n",
    "        epochs=10, \n",
    "        batch_size=32,\n",
    "        validation_data=(x_test, y_test))\n",
    "    \n",
    "    acc_filename = net + \"_d\" + str(depth) + \"_\" + str(pretrained_features.shape[0]) + \"_accuracy\"\n",
    "    loss_filename = net + \"_d\" + str(depth) + \"_\" + str(pretrained_features.shape[0]) + \"_loss\"\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.savefig(acc_filename)\n",
    "    plt.clf()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.savefig(loss_filename)\n",
    "    plt.clf()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single dense model for one pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143998 samples, validate on 16000 samples\n",
      "Epoch 1/10\n",
      "143998/143998 [==============================] - 19s 135us/sample - loss: 0.3117 - accuracy: 0.8749 - val_loss: 0.2836 - val_accuracy: 0.8882\n",
      "Epoch 2/10\n",
      "143998/143998 [==============================] - 19s 135us/sample - loss: 0.2711 - accuracy: 0.8962 - val_loss: 0.2501 - val_accuracy: 0.9068\n",
      "Epoch 3/10\n",
      "143998/143998 [==============================] - 19s 135us/sample - loss: 0.2592 - accuracy: 0.9028 - val_loss: 0.2427 - val_accuracy: 0.9102\n",
      "Epoch 4/10\n",
      "143998/143998 [==============================] - 19s 134us/sample - loss: 0.2528 - accuracy: 0.9053 - val_loss: 0.2348 - val_accuracy: 0.9128\n",
      "Epoch 5/10\n",
      "143998/143998 [==============================] - 19s 134us/sample - loss: 0.2478 - accuracy: 0.9077 - val_loss: 0.2400 - val_accuracy: 0.9110\n",
      "Epoch 6/10\n",
      "143998/143998 [==============================] - 19s 134us/sample - loss: 0.2475 - accuracy: 0.9086 - val_loss: 0.2400 - val_accuracy: 0.9112\n",
      "Epoch 7/10\n",
      "143998/143998 [==============================] - 19s 135us/sample - loss: 0.2443 - accuracy: 0.9097 - val_loss: 0.2425 - val_accuracy: 0.9096\n",
      "Epoch 8/10\n",
      "143998/143998 [==============================] - 19s 134us/sample - loss: 0.2429 - accuracy: 0.9097 - val_loss: 0.2312 - val_accuracy: 0.9149\n",
      "Epoch 9/10\n",
      "143998/143998 [==============================] - 19s 134us/sample - loss: 0.2405 - accuracy: 0.9107 - val_loss: 0.2482 - val_accuracy: 0.9094\n",
      "Epoch 10/10\n",
      "143998/143998 [==============================] - 19s 134us/sample - loss: 0.2403 - accuracy: 0.9117 - val_loss: 0.2363 - val_accuracy: 0.9108\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Single net testing\n",
    "\n",
    "net = \"VGG16\"\n",
    "depth = \"full\"\n",
    "epochs = 10\n",
    "# Load features\n",
    "features_filename = net + \"_d\" + str(depth) + \"_\" + str(images.shape[0]) + \".npy\"\n",
    "pretrained_features = load_feature_representation(features_filename)\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=pretrained_features.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Remove nan values from pretrained_features (and remove labels which produces them)\n",
    "nan_indices = []\n",
    "for i in range(pretrained_features.shape[0]):\n",
    "    if np.isnan(pretrained_features[i,:]).any():\n",
    "        nan_indices.append(i)\n",
    "pretrained_features = np.delete(pretrained_features, nan_indices, axis=0)\n",
    "tmp_labels = np.delete(labels, nan_indices, axis=0)\n",
    "tmp_positions = np.delete(positions, nan_indices, axis=0)\n",
    "\n",
    "labels_positions = np.concatenate((tmp_labels, tmp_positions), axis=1)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(pretrained_features, labels_positions, test_size = 0.2)    \n",
    "\n",
    "test_positions = y_test[:, 2:]\n",
    "train_positions = y_train[:, 2:]\n",
    "y_test = y_test[:, :2]\n",
    "y_train = y_train[:, :2]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=epochs, \n",
    "    batch_size=32,\n",
    "    validation_split=0.1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test set and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_predicted = model.predict(x_test)\n",
    "tmp_results = tmp_predicted.argmax(axis=-1)\n",
    "double_events = double_event_distance(tmp_results, test_positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean dist all double events:  9.767019782389875\n",
      "Mean dist correct double events:  10.360482266261874\n",
      "Mean dist double events:  6.89233866511357\n",
      "Ratio correctly classified events with dist < 3mm:  0.41183959261616804\n"
     ]
    }
   ],
   "source": [
    "# double_events contain (predicted_class, distance between event origins)\n",
    "# All the events are double evenets, so if predicted class is 0, the predicted\n",
    "# class is wrong.\n",
    "\n",
    "correct = np.where(double_events[:,0] == 1)\n",
    "wrong = np.where(double_events[:,0] == 0)\n",
    "\n",
    "mean_correct = np.mean(double_events[correct][:,1])\n",
    "mean_wrong = np.mean(double_events[wrong][:,1])\n",
    "mean_all = np.mean(double_events[:,1])\n",
    "\n",
    "# Ratio of correctly classified double events with a distance between\n",
    "# events < 3mm\n",
    "close_events = np.where(double_events[:,1] < 3.0)\n",
    "ratio_close = np.count_nonzero(double_events[close_events][:,0])/len(double_events[close_events][:,0])\n",
    "\n",
    "\n",
    "\n",
    "# Output\n",
    "print(\"Mean dist all double events: \", mean_all)\n",
    "print(\"Mean dist correct double events: \", mean_correct)\n",
    "print(\"Mean dist double events: \", mean_wrong)\n",
    "print(\"Ratio correctly classified events with dist < 3mm: \", ratio_close)\n",
    "\n",
    "del(tmp_predicted)\n",
    "del(tmp_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histograms\n",
    "plt.hist(double_events[:,1], bins=50)\n",
    "plt.xlabel(\"mm\")\n",
    "plt.ylabel(\"freq\")\n",
    "plt.title(\"Distances between all double events\")\n",
    "plt.show()\n",
    "plt.hist(double_events[correct][:,1], bins=50, alpha=0.5, label=\"correct\")\n",
    "plt.hist(double_events[wrong][:,1], bins=50, alpha=0.5, label=\"wrong\")\n",
    "plt.xlabel(\"mm\")\n",
    "plt.ylabel(\"freq\")\n",
    "plt.title(\"Distances between classified events\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pretrained_venv",
   "language": "python",
   "name": "pretrained_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
