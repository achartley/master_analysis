{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using pretrained, well-known models\n",
    "This notebook aims to create a set of benchmarks for the project, using well-known, thoroughly studied models.\n",
    "Either with pretrained weights, or training with new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "### 21.08.19\n",
    "Attempting to classify with VGG has not proven effective yet.\n",
    "Initially, the image data was note scaled at all. Implemented scaling in the\n",
    "import function, using min-max scaling of the value. This preserves the inherent\n",
    "intensity difference between images.\n",
    "\n",
    "The number of layers of VGG16 used is varied between 3 to 9 without noticable\n",
    "difference. Attempting to find out why, by analyzing the extracted features.\n",
    "The idea is that in order to classify, the feature distribution should be\n",
    "different for images containing single and double events.\n",
    "I first attempt this with manual qualitative inspection.\n",
    "\n",
    "Manual, qualitative inspection reveals that the distributions look very similar.\n",
    "Performing a quantitative study using Kolmogorov-Smirnov two-sample test,\n",
    "comparing the distribution for each feature.\n",
    "\n",
    "* For 1 block (depth 3), the pvalue returned from comparisons is 1.0 for all features.\n",
    "* For 2 blocks (depth 6), the pvalue returned from comparisons is 1.0 for all features.\n",
    "* For 3 blocks (depth 10), the pvalue returned from comparisons is 1.0 for all features.\n",
    "* For 4 blocks, (depth 14) the pvalue returned from comparisons is 1.0 for all features.\n",
    "* For 5 blocks, (depth 18) the pvalue returned from comparisons is 1.0 for all features.\n",
    "\n",
    "This indicates that extracting features using vgg16 doesn't work for classification.\n",
    "I still want to confirm that the weights of the vgg layers are the imagenet weights.\n",
    "\n",
    "### 22.08.19\n",
    "Going to use a reference image from imagenet to verify that the vgg-layers behave\n",
    "as expected.\n",
    "* Reference produces very similar feature output as simulated data\n",
    "\n",
    "Rewrote the vgg_model script to be able to import any pretrained model from\n",
    "tensorflow, and extended data import to handle single files (for large file)\n",
    "and possibility to specify number of samples to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels shape:  (10000, 1)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'CeBr10k_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a5c1dd2671c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m                          \u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                         )\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"images\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0menergies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"energies\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mpositions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"positions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'CeBr10k_1'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import master_data_functions.functions as dfunc\n",
    "\n",
    "# silence deprecation warnings from tensorflow\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "\n",
    "# File import\n",
    "# Sample filenames are:\n",
    "# CeBr10kSingle_1.txt -> single events, \n",
    "# CeBr10kSingle_2.txt -> single events\n",
    "# CeBr10k_1.txt -> mixed single and double events \n",
    "# CeBr10.txt -> small file of 10 samples\n",
    "# CeBr2Mil_Mix.txt -> 2 million mixed samples of simulated events\n",
    "\n",
    "sample_keywords = {\n",
    "    \"single_1\": \"CeBr10kSingle_1.txt\",\n",
    "    \"single_2\": \"CeBr10kSingle_2.txt\",\n",
    "    \"mix\": \"CeBr10k_1.txt\",\n",
    "    \"small\": \"CeBr10.txt\"}\n",
    "    \n",
    "dataset = sample_keywords[\"mix\"]\n",
    "data = dfunc.import_data(folder=\"sample\", \n",
    "                         filename=\"CeBr10k_1.txt\", \n",
    "                         num_samples=1e3,\n",
    "                         scaling=True\n",
    "                        )\n",
    "images = data[dataset][\"images\"]\n",
    "energies = data[dataset][\"energies\"]\n",
    "positions = data[dataset][\"positions\"]\n",
    "labels = to_categorical(data[dataset][\"labels\"])\n",
    "n_classes = labels.shape[1]\n",
    "\n",
    "\n",
    "print(\"Number of classes: {}\".format(n_classes))\n",
    "print(\"Energies shape: {}\".format(energies.shape))\n",
    "print(\"Positions shape: {}\".format(positions.shape))\n",
    "print(\"Labels shape: {}\".format(labels.shape))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16 expects 3 channels. Solving this by concatenating the image data \n",
    "# to itself, to form three identical channels\n",
    "\n",
    "images = np.concatenate((images, images, images), axis=3)\n",
    "\n",
    "# Load a reference image\n",
    "\n",
    "\n",
    "\n",
    "print(\"Image data shape: {}\".format(images.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_image = plt.imread(\"../../data/reference_cabin.png\")\n",
    "ref_image = ref_image.reshape((1,)+ref_image.shape)\n",
    "print(ref_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-16 feature extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from master_models.pretrained import vgg_model\n",
    "vgg = vgg_model(output_depth=10)\n",
    "vgg.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract features with vgg\n",
    "vgg_features = vgg.predict(images)\n",
    "\n",
    "\n",
    "ref_vgg_features = vgg.predict(ref_image)\n",
    "print(ref_vgg_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check distribution of features\n",
    "\n",
    "manual_inspect = False\n",
    "\n",
    "single_features = vgg_features[np.where(labels[:,0] == 1)]\n",
    "double_features = vgg_features[np.where(labels[:,1] == 1)]\n",
    "\n",
    "#plt.plot(single_features[0,:], alpha=0.5, label='single')\n",
    "plt.plot(double_features[0,:], alpha=0.5, label='double')\n",
    "plt.plot(ref_vgg_features[0,:], alpha=0.5, label='reference')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "if manual_inspect:\n",
    "    index = 0 \n",
    "    fig, ax = plt.subplots(3, 3, sharex='col', sharey='row', figsize=(12,12))\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            # plot features\n",
    "            ax[i, j].hist(single_features[:,index + i*3 + j], alpha=0.5, label='single')\n",
    "            ax[i, j].hist(double_features[:,index + i*3 + j], alpha=0.5, label='double')\n",
    "            ax[i, j].hist(ref_vgg_features, alpha=0.5, label='reference')\n",
    "            ax[i, j].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "from joblib import Parallel, delayed\n",
    "# Check difference using Kolmogorov-Smirnov\n",
    "\n",
    "def get_pval(i):\n",
    "    ks = ks_2samp(single_features[:,i], double_features[:,i])\n",
    "    return ks.pvalue\n",
    "\n",
    "n = vgg_features.shape[1]\n",
    "p_values = Parallel(n_jobs=-1, verbose=2)(delayed(get_pval)(i) for i in range(n))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(p_values)), p_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with custom dense network\n",
    "### Build dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a fully-connected network to classify based on\n",
    "# extracted features\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "model = Sequential()\n",
    "model.add(Dense(4096, input_shape=vgg_features.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(vgg_features, labels, test_size = 0.2)\n",
    "print(\"Training and test data shapes:\")\n",
    "print(\"x_train: {}\".format(x_train.shape))\n",
    "print(\"x_test: {}\".format(x_test.shape))\n",
    "print(\"y_train: {}\".format(y_train.shape))\n",
    "print(\"y_test: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=10, \n",
    "    batch_size=128,\n",
    "    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check predictions\n",
    "predicted = model.predict(x_test)\n",
    "print(len(predicted[np.where(predicted[:,1] == 1.0)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pretrained_venv",
   "language": "python",
   "name": "pretrained_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
