{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using pretrained, well-known models\n",
    "This notebook aims to create a set of benchmarks for the project, using well-known, thoroughly studied models.\n",
    "Either with pretrained weights, or training with new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "### 21.08.19\n",
    "Attempting to classify with VGG has not proven effective yet.\n",
    "Initially, the image data was note scaled at all. Implemented scaling in the\n",
    "import function, using min-max scaling of the value. This preserves the inherent\n",
    "intensity difference between images.\n",
    "\n",
    "The number of layers of VGG16 used is varied between 3 to 9 without noticable\n",
    "difference. Attempting to find out why, by analyzing the extracted features.\n",
    "The idea is that in order to classify, the feature distribution should be\n",
    "different for images containing single and double events.\n",
    "I first attempt this with manual qualitative inspection.\n",
    "\n",
    "Manual, qualitative inspection reveals that the distributions look very similar.\n",
    "Performing a quantitative study using Kolmogorov-Smirnov two-sample test,\n",
    "comparing the distribution for each feature.\n",
    "\n",
    "* For 1 block (depth 3), the pvalue returned from comparisons is 1.0 for all features.\n",
    "* For 2 blocks (depth 6), the pvalue returned from comparisons is 1.0 for all features.\n",
    "* For 3 blocks (depth 10), the pvalue returned from comparisons is 1.0 for all features.\n",
    "* For 4 blocks, (depth 14) the pvalue returned from comparisons is 1.0 for all features.\n",
    "* For 5 blocks, (depth 18) the pvalue returned from comparisons is 1.0 for all features.\n",
    "\n",
    "This indicates that extracting features using vgg16 doesn't work for classification.\n",
    "I still want to confirm that the weights of the vgg layers are the imagenet weights.\n",
    "\n",
    "### 22.08.19\n",
    "Going to use a reference image from imagenet to verify that the vgg-layers behave\n",
    "as expected.\n",
    "* Reference produces very similar feature output as simulated data\n",
    "\n",
    "Rewrote the vgg_model script to be able to import any pretrained model from\n",
    "tensorflow, and extended data import to handle single files (for large file)\n",
    "and possibility to specify number of samples to include.\n",
    "\n",
    "### 23.08.19\n",
    "Import scripts fixed so that array dimensions are correct independent of folder or single\n",
    "file import.\n",
    "\n",
    "Tests on multiple nets with 10k events give same results as for VGG.\n",
    "\n",
    "### 24.08.19\n",
    "Running checks on feature distribution with the full networks and 200k events.\n",
    "With 200k events there are models which from the p-value given by the KS two-sample test\n",
    "should be possible to classify with. Interstingly, VGG16 and VGG19 show a large variance in which\n",
    "features are seemingly drawn from different distributions.\n",
    "\n",
    "Need to run for: NASNetLarge, ResNet50 and Xception, but running into memory problems.\n",
    "\n",
    "### 25.08.19\n",
    "Some NaN values in output features from VGG. When removed, the network trains to acc = 0.9\n",
    "on the features. \n",
    "\n",
    "Implemented save_feature_representation and load_feature_representation.\n",
    "\n",
    "### 04.09.19\n",
    "Storing 200k events to use for generating feature representations for classification.\n",
    "\n",
    "### 09.09.19\n",
    "Previously trained fully connected nets on the feature output from all pretrained networks available\n",
    "at full depth. The fully-connected nets were configured so that the input layer had a number of nodes\n",
    "equal to the number of features output from the convolutional block before it (because the input shape\n",
    "must be defined). After that, two layers of 512 nodes with RELu actication functions, into one\n",
    "2-node layer with softmax.\n",
    "Results:\n",
    "* __DenseNet121__: \n",
    "    Accuracy starts around 0.86 first epoch and rises to just under 0.94 at maximum.\n",
    "    Somewhat unstable.\n",
    "* __DenseNet169__: \n",
    "    Very similar to DenseNet121, but starts at around 0.90\n",
    "* __DenseNet201__: \n",
    "    Very similar to other two DenseNets, but more stable above 0.92 for epoch 4+.\n",
    "* __InceptionResNetV2__: \n",
    "    Starts at 0.84, maxes just above 0.88. A bit zig-zag.\n",
    "* __InceptionV3__: \n",
    "    Similar to V2 above, but starts lower and maxes out higher (0.90). Huge dip at epoch 7,\n",
    "    around same spot as DenseNets.\n",
    "* __MobileNet__: \n",
    "    Didn't learn anything.\n",
    "* __MobileNetV2__: \n",
    "    Didn't learn anything.\n",
    "* __NASNetLarge__: \n",
    "    Starts at 0.88, maxes out at 0.92 ish. Stable nice increase in accuracy with small dip at last epoch.\n",
    "* __NASNetMobile__: \n",
    "    Roughly same as NASNetLarge, but a little more zig-zag in the curve.\n",
    "* __ResNet50__: \n",
    "    Didn't learn anything.\n",
    "* __VGG16__: \n",
    "    A lot of zig-zag, but between 0.88 and just under 0.92 (max), so it's a small span.\n",
    "    Stops zig-zagging on max and becomes pretty much flat.\n",
    "* __VGG19__: \n",
    "    Smaller zig-zag distances, steady increase up to around 0.90 for maximum.\n",
    "* __Xception__: \n",
    "    Starts high (0.88) and slowly rises to a maximum of 0.92, a dip around epoch 6 like many others.\n",
    "    \n",
    "From this I think the nets that didn't weren't able to extract anything from the features can be\n",
    "ignored for a while, to focus on those that performed well. This leaves the following nets as possible\n",
    "points of interest:\n",
    "* DenseNet201 (best of the DenseNets, keep the others in mind)\n",
    "* NASNetLarge (keep NASNetMobile in mind)\n",
    "* VGG16\n",
    "* VGG19\n",
    "* Xception\n",
    "\n",
    "### 15.09.19\n",
    "Simple implementation of distance-checking of double events done.\n",
    "Results indicate that the model struggles more with events that are close together.\n",
    "Specifically, I've looked at the ratio of correctly classified double events where\n",
    "the events are less than 3mm apart.\n",
    "\n",
    "### 16.09.19\n",
    "* Compare feature distributions of single events and double events with distance < 3mm.\n",
    "Plots produced. There is a similar difference between single events and close double events,\n",
    "as between single events and double events in general.\n",
    "\n",
    "* Is the number of close events large enough to be confident in the result of the KS two-sample test?\n",
    "* What does the above result imply?\n",
    "* Is weighting the training data such that close events are weighted higher than others a good idea? Would this introduce a bias?\n",
    "\n",
    "### 18.09.19\n",
    "Producing histograms of correct vs wrong classifications for predictions on double events, specifically.\n",
    "This gives additional insight in how well the models classify events, especially compared with the total\n",
    "accuracy. An example is VGG16: The total accuracy for classification is around 0.88-0.92, but when looking\n",
    "at event with a distance lower than 3mm between them, the ratio of correctly classified close events is \n",
    "around 0.4. Densenet201 full depth has comparable total accuracy, but a ratio for close events at 0.72, \n",
    "which is significantly better.\n",
    "\n",
    "### 23.09.19\n",
    "Distance calculations have been corrected. Relative energy implemented.\n",
    "MobileNet and ResNet50 aren't able to learn anything\n",
    "\n",
    "### 24.09.19\n",
    "Produced and wrote about feature distribution comparisons.\n",
    "Not done, but it's a start.\n",
    "\n",
    "### 25.09.19\n",
    "Implemented confusion matrix plotter (from scikit-learn's example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "* Implement functions to save and load trained fully-connected networks\n",
    "* k-fold cross validation solves the problem with differences in training and test data.\n",
    "* Plot events that are difficult. Perhaps try to find events that no networks can classify?\n",
    "    * Can pass a set of indices along with data to train_test_split\n",
    "* Confusion matrix plots\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from master_data_functions.functions import import_data,save_feature_representation,load_feature_representation\n",
    "from master_models.pretrained import pretrained_model\n",
    "from master_data_functions.functions import event_indices, relative_distance, relative_energy\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 2\n",
      "Images shape: (200000, 16, 16, 1)\n",
      "Energies shape: (200000, 2)\n",
      "Positions shape: (200000, 4)\n",
      "Labels shape: (200000, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# File import\n",
    "# Sample filenames are:\n",
    "# CeBr10kSingle_1.txt -> single events, \n",
    "# CeBr10kSingle_2.txt -> single events\n",
    "# CeBr10k_1.txt -> mixed single and double events \n",
    "# CeBr10.txt -> small file of 10 samples\n",
    "# CeBr2Mil_Mix.txt -> 2 million mixed samples of simulated events\n",
    "\n",
    "# Flag import, since we can now import 200k events from .npy files\n",
    "from_file = False\n",
    "if from_file:\n",
    "\n",
    "    folder = \"simulated\"\n",
    "    filename = \"CeBr2Mil_Mix.txt\"\n",
    "    num_samples = 2e5\n",
    "    #folder = \"sample\"\n",
    "    #filename = \"CeBr10k_1.txt\"\n",
    "    #num_samples = 1e3\n",
    "\n",
    "    data = import_data(folder=folder, filename=filename, num_samples=num_samples)\n",
    "    images = data[filename][\"images\"]\n",
    "    energies = data[filename][\"energies\"]\n",
    "    positions = data[filename][\"positions\"]\n",
    "    labels = to_categorical(data[filename][\"labels\"])\n",
    "    n_classes = labels.shape[1]\n",
    "else:\n",
    "    images = load_feature_representation(\"images_200k.npy\")\n",
    "    energies = load_feature_representation(\"energies_200k.npy\")\n",
    "    positions = load_feature_representation(\"positions_200k.npy\")\n",
    "    labels = load_feature_representation(\"labels_200k.npy\")\n",
    "\n",
    "n_classes = labels.shape[1]\n",
    "print(\"Number of classes: {}\".format(n_classes))\n",
    "print(\"Images shape: {}\".format(images.shape))\n",
    "print(\"Energies shape: {}\".format(energies.shape))\n",
    "print(\"Positions shape: {}\".format(positions.shape))\n",
    "print(\"Labels shape: {}\".format(labels.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image data shape: (200000, 16, 16, 3)\n"
     ]
    }
   ],
   "source": [
    "# VGG16 expects 3 channels. Solving this by concatenating the image data \n",
    "# to itself, to form three identical channels\n",
    "\n",
    "images = np.concatenate((images, images, images), axis=3)\n",
    "print(\"Image data shape: {}\".format(images.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with custom dense network\n",
    "### Multiple dense model\n",
    "Build a dense model for each pretrained model form which a feature representation has been saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keys: model names, Values: depth to compare at.\n",
    "pretrained_models = {\n",
    "    \"DenseNet121\":None, #8\n",
    "    \"DenseNet169\":None, #8\n",
    "    \"DenseNet201\":None, #8\n",
    "    \"InceptionResNetV2\":None, #8\n",
    "    \"InceptionV3\":None, #8\n",
    "    \"MobileNet\":None, #8\n",
    "    \"MobileNetV2\":None, #5\n",
    "    \"NASNetLarge\":None, #4\n",
    "    \"NASNetMobile\":None, #4\n",
    "    \"ResNet50\":None, #8\n",
    "    \"VGG16\":None,\n",
    "    \"VGG19\":None,\n",
    "    \"Xception\":None, #6\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training, and plotting of accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train multiple fully-connected networks to classify based on\n",
    "# extracted features\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "# Set up and train a network for each of the pretrained features available    \n",
    "for net, depth in pretrained_models.items():\n",
    "    print(\"Running for:\", net)\n",
    "    \n",
    "    # Load features\n",
    "    if depth is None:\n",
    "        depth = \"full\"\n",
    "    features_filename = net + \"_d\" + str(depth) + \"_\" + str(images.shape[0]) + \".npy\"\n",
    "    pretrained_features = load_feature_representation(features_filename)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=pretrained_features.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Remove nan values from pretrained_features (and remove labels which produces them)\n",
    "    nan_indices = []\n",
    "    for i in range(pretrained_features.shape[0]):\n",
    "        if np.isnan(pretrained_features[i,:]).any():\n",
    "            nan_indices.append(i)\n",
    "    pretrained_features = np.delete(pretrained_features, nan_indices, axis=0)\n",
    "    tmp_labels = np.delete(labels, nan_indices, axis=0)\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(pretrained_features, tmp_labels, test_size = 0.2)    \n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x_train, \n",
    "        y_train, \n",
    "        epochs=10, \n",
    "        batch_size=32,\n",
    "        validation_data=(x_test, y_test))\n",
    "    \n",
    "    acc_filename = net + \"_d\" + str(depth) + \"_\" + str(pretrained_features.shape[0]) + \"_accuracy\"\n",
    "    loss_filename = net + \"_d\" + str(depth) + \"_\" + str(pretrained_features.shape[0]) + \"_loss\"\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.savefig(acc_filename)\n",
    "    plt.clf()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.savefig(loss_filename)\n",
    "    plt.clf()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single dense model for one pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0925 20:24:54.678203 140078319314752 deprecation.py:323] From /home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143998 samples, validate on 16000 samples\n",
      "Epoch 1/5\n",
      "143998/143998 [==============================] - 25s 171us/sample - loss: 0.3230 - accuracy: 0.8626 - val_loss: 0.3772 - val_accuracy: 0.8441\n",
      "Epoch 2/5\n",
      "143998/143998 [==============================] - 22s 152us/sample - loss: 0.2596 - accuracy: 0.8986 - val_loss: 0.2633 - val_accuracy: 0.8975\n",
      "Epoch 3/5\n",
      "143998/143998 [==============================] - 22s 150us/sample - loss: 0.2318 - accuracy: 0.9125 - val_loss: 0.2051 - val_accuracy: 0.9268\n",
      "Epoch 4/5\n",
      "143998/143998 [==============================] - 24s 170us/sample - loss: 0.2168 - accuracy: 0.9208 - val_loss: 0.2089 - val_accuracy: 0.9259\n",
      "Epoch 5/5\n",
      "143998/143998 [==============================] - 24s 163us/sample - loss: 0.2078 - accuracy: 0.9249 - val_loss: 0.1975 - val_accuracy: 0.9281\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Single net testing\n",
    "\n",
    "net = \"DenseNet201\"\n",
    "depth = \"full\"\n",
    "epochs = 5\n",
    "# Load features\n",
    "features_filename = net + \"_d\" + str(depth) + \"_\" + str(images.shape[0]) + \".npy\"\n",
    "pretrained_features = load_feature_representation(features_filename)\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=pretrained_features.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Remove nan values from pretrained_features (and remove labels which produces them)\n",
    "nan_indices = []\n",
    "for i in range(pretrained_features.shape[0]):\n",
    "    if np.isnan(pretrained_features[i,:]).any():\n",
    "        nan_indices.append(i)\n",
    "pretrained_features = np.delete(pretrained_features, nan_indices, axis=0)\n",
    "tmp_labels = np.delete(labels, nan_indices, axis=0)\n",
    "tmp_positions = np.delete(positions, nan_indices, axis=0)\n",
    "tmp_energies = np.delete(energies, nan_indices, axis=0)\n",
    "\n",
    "labels_positions_energies = np.concatenate((tmp_labels, tmp_positions, tmp_energies), axis=1)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(pretrained_features, labels_positions_energies, test_size = 0.2)    \n",
    "\n",
    "test_positions = y_test[:, 2:6]\n",
    "train_positions = y_train[:, 2:6]\n",
    "test_energies = y_test[:, 6:]\n",
    "train_energies = y_train[:, 6:]\n",
    "y_test = y_test[:, :2]\n",
    "y_train = y_train[:, :2]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=epochs, \n",
    "    batch_size=32,\n",
    "    validation_split=0.1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test set and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_predicted = model.predict(x_test)\n",
    "tmp_results = tmp_predicted.argmax(axis=-1).reshape(tmp_predicted.shape[0], 1)\n",
    "\n",
    "# indices, relative distances and relative energies for test set\n",
    "single_indices, double_indices, close_indices = event_indices(test_positions)\n",
    "rel_distance = relative_distance(test_positions)\n",
    "rel_energy = relative_energy(test_energies)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[20051    53]\n",
      " [ 2773 17123]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f657e7254e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from analysis_functions.plotting import plot_confusion_matrix\n",
    "classes = [\"single\", \"double\"]\n",
    "plot_confusion_matrix(y_test.argmax(axis=-1), tmp_results, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_doubles = np.where(tmp_results[double_indices] == 1)[0]\n",
    "print(len(np.where(rel_energy[double_indices] != -100)[0]))\n",
    "print(len(np.where(rel_energy[double_indices][correct_doubles] == -100)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Separate correct and wrong classifications\n",
    "\n",
    "\n",
    "correct_doubles = np.where(tmp_results[double_indices] == 1)[0]\n",
    "wrong_doubles = np.where(tmp_results[double_indices] == 0)[0]\n",
    "\n",
    "mean_correct = np.mean(rel_distance[double_indices][correct_doubles])\n",
    "mean_wrong = np.mean(rel_distance[double_indices][wrong_doubles])\n",
    "mean_all = np.mean(rel_distance[double_indices])\n",
    "\n",
    "ratio_doubles = len(correct_doubles) / len(double_indices)\n",
    "\n",
    "# Ratio of correctly classified double events with a distance between\n",
    "# events < 3mm\n",
    "n_close = len(close_indices)\n",
    "n_close_correct = len(np.where(rel_distance[double_indices][correct_doubles] < 3.0)[0])\n",
    "ratio_close = n_close_correct / n_close\n",
    "print(len(correct_doubles))\n",
    "print(len(wrong_doubles))\n",
    "\n",
    "# Output\n",
    "print(\"Mean dist all double events: \", mean_all)\n",
    "print(\"Mean dist correct double events: \", mean_correct)\n",
    "print(\"Mean dist wrong double events: \", mean_wrong)\n",
    "print(\"Ratio of correctly classified double events: \", ratio_doubles)\n",
    "print(\"Ratio correctly classified events with dist < 3mm: \", ratio_close)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Histograms\n",
    "dist_bins = np.arange(0, np.amax(rel_distance), 0.5)\n",
    "energy_bins = np.arange(0, np.amax(rel_energy), 0.02)\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12,8))\n",
    "ax[0,0].hist(rel_distance[double_indices][correct_doubles], bins=dist_bins, alpha=0.5, label=\"correct\")\n",
    "ax[0,0].hist(rel_distance[double_indices][wrong_doubles], bins=dist_bins, alpha=0.5, label=\"wrong\")\n",
    "ax[0,0].set_title(\"Relative distance\")\n",
    "ax[0,0].legend()\n",
    "ax[0,1].hist(rel_energy[double_indices][correct_doubles], bins=energy_bins, alpha=0.5, label=\"correct\")\n",
    "ax[0,1].hist(rel_energy[double_indices][wrong_doubles], bins=energy_bins, alpha=0.5, label=\"wrong\")\n",
    "ax[0,1].set_title(\"Relative energy\")\n",
    "ax[0,1].legend()\n",
    "ax[1,0].hist(rel_distance[double_indices], bins=50)\n",
    "ax[1,0].set_title(\"Distribution of relative distances\")\n",
    "ax[1,1].hist(rel_energy[double_indices], bins=50)\n",
    "ax[1,1].set_title(\"Distribution of relative energies\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot examples of events that are ~always misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "for index in wrong_doubles:\n",
    "    if index in close_indices:\n",
    "        indices.append(index)\n",
    "\n",
    "\n",
    "img_plot = images.reshape(images.shape[0], 16, 16)\n",
    "fig, ax = plt.subplots(3, 3, sharex='col', sharey='row', figsize=(12,12))\n",
    "\n",
    "index = 0\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        # plot image\n",
    "        ax[i, j].imshow(img_plot[indices[i*3 + j]])\n",
    "        \n",
    "        # plot origin of event\n",
    "        x = test_positions[indices[index + i*3 + j], 0]\n",
    "        y = test_positions[indices[index + i*3 + j], 1]\n",
    "        ax[i, j].plot(x, y, 'rx')\n",
    "        ax[i, j].set_title('single')\n",
    "        if test_positions[indices[index + i*3 + j], 3] != -100:\n",
    "            x2 = test_positions[indices[index + i*3 + j], 2]\n",
    "            y2 = test_positions[indices[index + i*3 + j], 3]\n",
    "            ax[i, j].plot(x2, y2, 'rx')\n",
    "            ax[i, j].set_title('double')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some images, with electron origin positions\n",
    "%matplotlib inline\n",
    "\n",
    "images = images.reshape(images.shape[0],16,16)\n",
    "\n",
    "fig, ax = plt.subplots(3, 3, sharex='col', sharey='row', figsize=(12,12))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        # plot image\n",
    "        ax[i, j].imshow(images[index + i*3 + j])\n",
    "        \n",
    "        # plot origin of event\n",
    "        x = positions[index + i*3 + j, 0]\n",
    "        y = positions[index + i*3 + j, 1]\n",
    "        ax[i, j].plot(x, y, 'rx')\n",
    "        ax[i, j].set_title('single')\n",
    "        if positions[index + i*3 + j, 3] != -100:\n",
    "            x2 = positions[index + i*3 + j, 2]\n",
    "            y2 = positions[index + i*3 + j, 3]\n",
    "            ax[i, j].plot(x2, y2, 'rx')\n",
    "            ax[i, j].set_title('double')\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set up and train a network for each of the pretrained features available \n",
    "# and output histograms of correct vs incorrect classifications\n",
    "for net, depth in pretrained_models.items():\n",
    "    print(\"Running for:\", net)\n",
    "    \n",
    "    # Load features\n",
    "    if depth is None:\n",
    "        depth = \"full\"\n",
    "    features_filename = net + \"_d\" + str(depth) + \"_\" + str(images.shape[0]) + \".npy\"\n",
    "    pretrained_features = load_feature_representation(features_filename)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=pretrained_features.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Remove nan values from pretrained_features (and remove labels which produces them)\n",
    "    nan_indices = []\n",
    "    for i in range(pretrained_features.shape[0]):\n",
    "        if np.isnan(pretrained_features[i,:]).any():\n",
    "            nan_indices.append(i)\n",
    "    pretrained_features = np.delete(pretrained_features, nan_indices, axis=0)\n",
    "    tmp_labels = np.delete(labels, nan_indices, axis=0)\n",
    "    tmp_positions = np.delete(positions, nan_indices, axis=0)\n",
    "    tmp_energies = np.delete(energies, nan_indices, axis=0)\n",
    "\n",
    "    labels_positions_energies = np.concatenate((tmp_labels, tmp_positions, tmp_energies), axis=1)\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(pretrained_features, labels_positions_energies, test_size = 0.2)    \n",
    "\n",
    "    test_positions = y_test[:, 2:6]\n",
    "    train_positions = y_train[:, 2:6]\n",
    "    test_energies = y_test[:, 6:]\n",
    "    train_energies = y_train[:, 6:]\n",
    "    y_test = y_test[:, :2]\n",
    "    y_train = y_train[:, :2] \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x_train, \n",
    "        y_train, \n",
    "        epochs=5, \n",
    "        batch_size=32,\n",
    "        validation_split=0.1)\n",
    "    \n",
    "    tmp_predicted = model.predict(x_test)\n",
    "    tmp_results = tmp_predicted.argmax(axis=-1).reshape(tmp_predicted.shape[0], 1)\n",
    "\n",
    "    # indices, relative distances and relative energies for test set\n",
    "    single_indices, double_indices, close_indices = event_indices(test_positions)\n",
    "    rel_distance = relative_distance(test_positions)\n",
    "    rel_energy = relative_energy(test_energies)\n",
    "    \n",
    "    # double_events contain (predicted_class, distance between event origins)\n",
    "    # All the events are double events, so if predicted class is 0, the predicted\n",
    "    # class is wrong.\n",
    "\n",
    "    correct_doubles = np.where(tmp_results[double_indices] == 1)[0]\n",
    "    wrong_doubles = np.where(tmp_results[double_indices] == 0)[0]\n",
    "\n",
    "    mean_correct = np.mean(rel_distance[double_indices][correct_doubles])\n",
    "    mean_wrong = np.mean(rel_distance[double_indices][wrong_doubles])\n",
    "    mean_all = np.mean(rel_distance[double_indices])\n",
    "\n",
    "    ratio_doubles = len(correct_doubles) / len(double_indices)\n",
    "\n",
    "    # Ratio of correctly classified double events with a distance between\n",
    "    # events < 3mm\n",
    "    n_close = len(close_indices)\n",
    "    n_close_correct = len(np.where(rel_distance[double_indices][correct_doubles] < 3.0)[0])\n",
    "    ratio_close = n_close_correct / n_close\n",
    "\n",
    "    del(tmp_predicted)\n",
    "    del(tmp_results)\n",
    "    # Output\n",
    "\n",
    "    # Histograms\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(12,8))\n",
    "    ax[0,0].hist(rel_distance[double_indices][correct_doubles], bins=50, alpha=0.5, label=\"correct\")\n",
    "    ax[0,0].hist(rel_distance[double_indices][wrong_doubles], bins=50, alpha=0.5, label=\"wrong\")\n",
    "    ax[0,0].set_title(\"Relative distance\")\n",
    "    ax[0,0].legend()\n",
    "    ax[0,1].hist(rel_energy[double_indices][correct_doubles], bins=50, alpha=0.5, label=\"correct\")\n",
    "    ax[0,1].hist(rel_energy[double_indices][wrong_doubles], bins=50, alpha=0.5, label=\"wrong\")\n",
    "    ax[0,1].set_title(\"Relative energy\")\n",
    "    ax[0,1].legend()\n",
    "    ax[1,0].hist(rel_distance[double_indices], bins=50)\n",
    "    ax[1,0].set_title(\"Distribution of relative distances\")\n",
    "    ax[1,1].hist(rel_energy[double_indices], bins=50)\n",
    "    ax[1,1].set_title(\"Distribution of relative energies\")\n",
    "    #ax[1].text(0, 600, r'  $\\mu$ correct = {:.2f} mm'.format(mean_correct))\n",
    "    #ax[1].text(0, 550, r'  $\\mu$ wrong = {:.2f} mm'.format(mean_wrong))\n",
    "    #ax[1].text(0, 500, r'  ratio doubles < 3mm = {:.2f}'.format(ratio_doubles))\n",
    "    #ax[1].text(0, 650, r'  $\\mu$ all = {:.2f} mm'.format(mean_all))\n",
    "    #ax[1].set_title(\"Numbers\")\n",
    "    figure_name = net + \"_d\" + str(depth) + \"_\" + str(images.shape[0]) + \"_relative.png\"\n",
    "    fig.savefig(figure_name, format=\"png\")\n",
    "    fig.clf()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pretrained_venv",
   "language": "python",
   "name": "pretrained_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
