{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using pretrained, well-known models\n",
    "This notebook aims to create a set of benchmarks for the project, using well-known, thoroughly studied models.\n",
    "Either with pretrained weights, or training with new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "### 21.08.19\n",
    "Attempting to classify with VGG has not proven effective yet.\n",
    "Initially, the image data was note scaled at all. Implemented scaling in the\n",
    "import function, using min-max scaling of the value. This preserves the inherent\n",
    "intensity difference between images.\n",
    "\n",
    "The number of layers of VGG16 used is varied between 3 to 9 without noticable\n",
    "difference. Attempting to find out why, by analyzing the extracted features.\n",
    "The idea is that in order to classify, the feature distribution should be\n",
    "different for images containing single and double events.\n",
    "I first attempt this with manual qualitative inspection.\n",
    "\n",
    "Manual, qualitative inspection reveals that the distributions look very similar.\n",
    "Performing a quantitative study using Kolmogorov-Smirnov two-sample test,\n",
    "comparing the distribution for each feature.\n",
    "\n",
    "* For 1 block (depth 3), the pvalue returned from comparisons is 1.0 for all features.\n",
    "* For 2 blocks (depth 6), the pvalue returned from comparisons is 1.0 for all features.\n",
    "* For 3 blocks (depth 10), the pvalue returned from comparisons is 1.0 for all features.\n",
    "* For 4 blocks, (depth 14) the pvalue returned from comparisons is 1.0 for all features.\n",
    "* For 5 blocks, (depth 18) the pvalue returned from comparisons is 1.0 for all features.\n",
    "\n",
    "This indicates that extracting features using vgg16 doesn't work for classification.\n",
    "I still want to confirm that the weights of the vgg layers are the imagenet weights.\n",
    "\n",
    "### 22.08.19\n",
    "Going to use a reference image from imagenet to verify that the vgg-layers behave\n",
    "as expected.\n",
    "* Reference produces very similar feature output as simulated data\n",
    "\n",
    "Rewrote the vgg_model script to be able to import any pretrained model from\n",
    "tensorflow, and extended data import to handle single files (for large file)\n",
    "and possibility to specify number of samples to include.\n",
    "\n",
    "### 23.08.19\n",
    "Import scripts fixed so that array dimensions are correct independent of folder or single\n",
    "file import.\n",
    "\n",
    "Tests on multiple nets with 10k events give same results as for VGG.\n",
    "\n",
    "### 24.08.19\n",
    "Running checks on feature distribution with the full networks and 200k events.\n",
    "With 200k events there are models which from the p-value given by the KS two-sample test\n",
    "should be possible to classify with. Interstingly, VGG16 and VGG19 show a large variance in which\n",
    "features are seemingly drawn from different distributions.\n",
    "\n",
    "Need to run for: NASNetLarge, ResNet50 and Xception, but running into memory problems.\n",
    "\n",
    "### 25.08.19\n",
    "Some NaN values in output features from VGG. When removed, the network trains to acc = 0.9\n",
    "on the features. \n",
    "\n",
    "Implemented save_feature_representation and load_feature_representation.\n",
    "\n",
    "### 04.09.19\n",
    "Storing 200k events to use for generating feature representations for classification.\n",
    "\n",
    "### 09.09.19\n",
    "Previously trained fully connected nets on the feature output from all pretrained networks available\n",
    "at full depth. The fully-connected nets were configured so that the input layer had a number of nodes\n",
    "equal to the number of features output from the convolutional block before it (because the input shape\n",
    "must be defined). After that, two layers of 512 nodes with RELu actication functions, into one\n",
    "2-node layer with softmax.\n",
    "Results:\n",
    "* __DenseNet121__: \n",
    "    Accuracy starts around 0.86 first epoch and rises to just under 0.94 at maximum.\n",
    "    Somewhat unstable.\n",
    "* __DenseNet169__: \n",
    "    Very similar to DenseNet121, but starts at around 0.90\n",
    "* __DenseNet201__: \n",
    "    Very similar to other two DenseNets, but more stable above 0.92 for epoch 4+.\n",
    "* __InceptionResNetV2__: \n",
    "    Starts at 0.84, maxes just above 0.88. A bit zig-zag.\n",
    "* __InceptionV3__: \n",
    "    Similar to V2 above, but starts lower and maxes out higher (0.90). Huge dip at epoch 7,\n",
    "    around same spot as DenseNets.\n",
    "* __MobileNet__: \n",
    "    Didn't learn anything.\n",
    "* __MobileNetV2__: \n",
    "    Didn't learn anything.\n",
    "* __NASNetLarge__: \n",
    "    Starts at 0.88, maxes out at 0.92 ish. Stable nice increase in accuracy with small dip at last epoch.\n",
    "* __NASNetMobile__: \n",
    "    Roughly same as NASNetLarge, but a little more zig-zag in the curve.\n",
    "* __ResNet50__: \n",
    "    Didn't learn anything.\n",
    "* __VGG16__: \n",
    "    A lot of zig-zag, but between 0.88 and just under 0.92 (max), so it's a small span.\n",
    "    Stops zig-zagging on max and becomes pretty much flat.\n",
    "* __VGG19__: \n",
    "    Smaller zig-zag distances, steady increase up to around 0.90 for maximum.\n",
    "* __Xception__: \n",
    "    Starts high (0.88) and slowly rises to a maximum of 0.92, a dip around epoch 6 like many others.\n",
    "    \n",
    "From this I think the nets that didn't weren't able to extract anything from the features can be\n",
    "ignored for a while, to focus on those that performed well. This leaves the following nets as possible\n",
    "points of interest:\n",
    "* DenseNet201 (best of the DenseNets, keep the others in mind)\n",
    "* NASNetLarge (keep NASNetMobile in mind)\n",
    "* VGG16\n",
    "* VGG19\n",
    "* Xception\n",
    "\n",
    "Things to get done before further exploration:\n",
    "* Implement functions to save and load trained fully-connected networks\n",
    "* Implement functions to extract the distance between events for classified double-events to\n",
    "    explore which ones are being classified wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from master_data_functions.functions import import_data,save_feature_representation,load_feature_representation\n",
    "from master_models.pretrained import pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 2\n",
      "Images shape: (200000, 16, 16, 1)\n",
      "Energies shape: (200000, 2)\n",
      "Positions shape: (200000, 4)\n",
      "Labels shape: (200000, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# File import\n",
    "# Sample filenames are:\n",
    "# CeBr10kSingle_1.txt -> single events, \n",
    "# CeBr10kSingle_2.txt -> single events\n",
    "# CeBr10k_1.txt -> mixed single and double events \n",
    "# CeBr10.txt -> small file of 10 samples\n",
    "# CeBr2Mil_Mix.txt -> 2 million mixed samples of simulated events\n",
    "\n",
    "# Flag import, since we can now import 200k events from .npy files\n",
    "from_file = False\n",
    "if from_file:\n",
    "\n",
    "    folder = \"simulated\"\n",
    "    filename = \"CeBr2Mil_Mix.txt\"\n",
    "    num_samples = 2e5\n",
    "    #folder = \"sample\"\n",
    "    #filename = \"CeBr10k_1.txt\"\n",
    "    #num_samples = 1e3\n",
    "\n",
    "    data = import_data(folder=folder, filename=filename, num_samples=num_samples)\n",
    "    images = data[filename][\"images\"]\n",
    "    energies = data[filename][\"energies\"]\n",
    "    positions = data[filename][\"positions\"]\n",
    "    labels = to_categorical(data[filename][\"labels\"])\n",
    "    n_classes = labels.shape[1]\n",
    "else:\n",
    "    images = load_feature_representation(\"images_200k.npy\")\n",
    "    energies = load_feature_representation(\"energies_200k.npy\")\n",
    "    positions = load_feature_representation(\"positions_200k.npy\")\n",
    "    labels = load_feature_representation(\"labels_200k.npy\")\n",
    "\n",
    "n_classes = labels.shape[1]\n",
    "print(\"Number of classes: {}\".format(n_classes))\n",
    "print(\"Images shape: {}\".format(images.shape))\n",
    "print(\"Energies shape: {}\".format(energies.shape))\n",
    "print(\"Positions shape: {}\".format(positions.shape))\n",
    "print(\"Labels shape: {}\".format(labels.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image data shape: (200000, 16, 16, 3)\n"
     ]
    }
   ],
   "source": [
    "# VGG16 expects 3 channels. Solving this by concatenating the image data \n",
    "# to itself, to form three identical channels\n",
    "\n",
    "images = np.concatenate((images, images, images), axis=3)\n",
    "print(\"Image data shape: {}\".format(images.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feature representations for all pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Keys: model names, Values: depth to compare at.\n",
    "pretrained_models = {\n",
    "    \"DenseNet121\":None, #8\n",
    "    \"DenseNet169\":None, #8\n",
    "    \"DenseNet201\":None, #8\n",
    "    \"InceptionResNetV2\":None, #8\n",
    "    \"InceptionV3\":None, #8\n",
    "    \"MobileNet\":None, #8\n",
    "    \"MobileNetV2\":None, #5\n",
    "    \"NASNetLarge\":None, #4\n",
    "    \"NASNetMobile\":None, #4\n",
    "    \"ResNet50\":None, #8\n",
    "    \"VGG16\":None,\n",
    "    \"VGG19\":None,\n",
    "    \"Xception\":None, #6\n",
    "    }                       \n",
    "\n",
    "for net, depth in pretrained_models.items():\n",
    "    print(\"Running for:\", net)\n",
    "    # Build net at desired depth\n",
    "    pretrained = pretrained_model(which_model=net, output_depth=depth)\n",
    "    \n",
    "    # Extract features and split them into single and double\n",
    "    pretrained_features = pretrained.predict(images)\n",
    "    \n",
    "    if depth is None:\n",
    "        depth = \"full\"\n",
    "    features_filename = net + \"_d\" + str(depth) + \"_\" + str(pretrained_features.shape[0]) + \"npy\"\n",
    "    save_feature_representation(pretrained_features, features_filename)\n",
    "    \n",
    "    # Delete to free memory for next iteration just in case\n",
    "    del pretrained_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run test on all pretrained nets.\n",
    "p_output = {}\n",
    "\n",
    "\n",
    "\n",
    "for net, depth in pretrained_models.items():\n",
    "    print(\"Running for:\", net)\n",
    "    \n",
    "    # Load features\n",
    "    if depth is None:\n",
    "        depth = \"full\"\n",
    "    features_filename = net + \"_d\" + str(depth) + \"_\" + str(pretrained_features.shape[0]) + \".npy\"\n",
    "    pretrained_features = load_feature_representation(features_filename)\n",
    "    single_features = pretrained_features[np.where(labels[:,0] == 1)]\n",
    "    double_features = pretrained_features[np.where(labels[:,1] == 1)]\n",
    "    n = pretrained_features.shape[1]\n",
    "    p_values = Parallel(n_jobs=-1, verbose=2)(delayed(get_pval)(i) for i in range(n))\n",
    "    p_output[net] = p_values\n",
    "    \n",
    "    plt.close()\n",
    "    plt.plot(range(len(p_values)), p_values, label=net)\n",
    "    plt.legend()\n",
    "    plt.savefig(net + \"-p-vals.png\")\n",
    "    #notify.send(net + \" finished!\")\n",
    "    \n",
    "    # Delete allocated arrays for saving memory in notebook\n",
    "    del pretrained_features\n",
    "    del single_features\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check features and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_inspect = False\n",
    "if manual_inspect:\n",
    "    # Compare feature output for reference image with a single and double image\n",
    "    plt.plot(range(len(reference_features[0])), reference_features[0], alpha=0.5, label='reference')\n",
    "    plt.plot(range(len(single_features[0])), single_features[0], alpha=0.5, label='single')\n",
    "    plt.plot(range(len(double_features[0])), double_features[0], alpha=0.5, label='double')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Check distribution of features by inspection\n",
    "    index = 0 \n",
    "    fig, ax = plt.subplots(3, 3, sharex='col', sharey='row', figsize=(12,12))\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            # plot features\n",
    "            ax[i, j].hist(single_features[:,index + i*3 + j], alpha=0.5, label='single')\n",
    "            ax[i, j].hist(double_features[:,index + i*3 + j], alpha=0.5, label='double')\n",
    "            ax[i, j].hist(ref_vgg_features, alpha=0.5, label='reference')\n",
    "            ax[i, j].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "from joblib import Parallel, delayed\n",
    "# Check difference using Kolmogorov-Smirnov\n",
    "\n",
    "def get_pval(i):\n",
    "    ks = ks_2samp(single_features[:,i], double_features[:,i])\n",
    "    return ks.pvalue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check feature distributions across all samples\n",
    "single_features = pretrained_features[np.where(labels[:,0] == 1)]\n",
    "double_features = pretrained_features[np.where(labels[:,1] == 1)]\n",
    "n = pretrained_features.shape[1]\n",
    "p_values = Parallel(n_jobs=-1, verbose=2)(delayed(get_pval)(i) for i in range(n))\n",
    "p_output[net] = p_values\n",
    "\n",
    "plt.close()\n",
    "plt.plot(range(len(p_values)), p_values, label=net)\n",
    "plt.legend()\n",
    "#plt.savefig(net + \"-p-vals.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with custom dense network\n",
    "### Build dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train a fully-connected network to classify based on\n",
    "# extracted features\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "# Set up and train a network for each of the pretrained features available    \n",
    "for net, depth in pretrained_models.items():\n",
    "    print(\"Running for:\", net)\n",
    "    \n",
    "    # Load features\n",
    "    if depth is None:\n",
    "        depth = \"full\"\n",
    "    features_filename = net + \"_d\" + str(depth) + \"_\" + str(images.shape[0]) + \".npy\"\n",
    "    pretrained_features = load_feature_representation(features_filename)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=pretrained_features.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Remove nan values from pretrained_features (and remove labels which produces them)\n",
    "    nan_indices = []\n",
    "    for i in range(pretrained_features.shape[0]):\n",
    "        if np.isnan(pretrained_features[i,:]).any():\n",
    "            nan_indices.append(i)\n",
    "    pretrained_features = np.delete(pretrained_features, nan_indices, axis=0)\n",
    "    tmp_labels = np.delete(labels, nan_indices, axis=0)\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(pretrained_features, tmp_labels, test_size = 0.2)    \n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x_train, \n",
    "        y_train, \n",
    "        epochs=10, \n",
    "        batch_size=32,\n",
    "        validation_data=(x_test, y_test))\n",
    "    \n",
    "    acc_filename = net + \"_d\" + str(depth) + \"_\" + str(pretrained_features.shape[0]) + \"_accuracy\"\n",
    "    loss_filename = net + \"_d\" + str(depth) + \"_\" + str(pretrained_features.shape[0]) + \"_loss\"\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.savefig(acc_filename)\n",
    "    plt.clf()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.savefig(loss_filename)\n",
    "    plt.clf()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159998 samples, validate on 40000 samples\n",
      "Epoch 1/10\n",
      "159998/159998 [==============================] - 23s 143us/sample - loss: 0.3124 - accuracy: 0.8741 - val_loss: 0.2511 - val_accuracy: 0.9071\n",
      "Epoch 2/10\n",
      "159998/159998 [==============================] - 23s 144us/sample - loss: 0.2693 - accuracy: 0.8977 - val_loss: 0.2418 - val_accuracy: 0.9106\n",
      "Epoch 3/10\n",
      "159998/159998 [==============================] - 23s 144us/sample - loss: 0.2612 - accuracy: 0.9011 - val_loss: 0.2393 - val_accuracy: 0.9140\n",
      "Epoch 4/10\n",
      "159998/159998 [==============================] - 23s 145us/sample - loss: 0.2543 - accuracy: 0.9049 - val_loss: 0.2919 - val_accuracy: 0.8868\n",
      "Epoch 5/10\n",
      "159998/159998 [==============================] - 23s 145us/sample - loss: 0.2504 - accuracy: 0.9065 - val_loss: 0.2387 - val_accuracy: 0.9116\n",
      "Epoch 6/10\n",
      "159998/159998 [==============================] - 23s 143us/sample - loss: 0.2462 - accuracy: 0.9084 - val_loss: 0.2792 - val_accuracy: 0.8905\n",
      "Epoch 7/10\n",
      "159998/159998 [==============================] - 23s 145us/sample - loss: 0.2454 - accuracy: 0.9095 - val_loss: 0.2332 - val_accuracy: 0.9170\n",
      "Epoch 8/10\n",
      "159998/159998 [==============================] - 23s 144us/sample - loss: 0.2435 - accuracy: 0.9099 - val_loss: 0.2309 - val_accuracy: 0.9158\n",
      "Epoch 9/10\n",
      "159998/159998 [==============================] - 23s 145us/sample - loss: 0.2422 - accuracy: 0.9101 - val_loss: 0.2429 - val_accuracy: 0.9079\n",
      "Epoch 10/10\n",
      "159998/159998 [==============================] - 23s 145us/sample - loss: 0.2397 - accuracy: 0.9120 - val_loss: 0.2267 - val_accuracy: 0.9187\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/ipykernel_launcher.py:45: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-de14a609111a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mtmp_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_predicted\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtmp_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mdouble_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdouble_event_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_positions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/master_scripts/data_functions/master_data_functions/functions.py\u001b[0m in \u001b[0;36mdouble_event_distance\u001b[0;34m(results, positions)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;31m#        distances[i] = -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0mdouble_distances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Single net testing\n",
    "from master_data_functions.functions import double_event_distance\n",
    "\n",
    "net = \"VGG16\"\n",
    "depth = \"full\"\n",
    "epochs = 2\n",
    "# Load features\n",
    "features_filename = net + \"_d\" + str(depth) + \"_\" + str(images.shape[0]) + \".npy\"\n",
    "pretrained_features = load_feature_representation(features_filename)\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=pretrained_features.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Remove nan values from pretrained_features (and remove labels which produces them)\n",
    "nan_indices = []\n",
    "for i in range(pretrained_features.shape[0]):\n",
    "    if np.isnan(pretrained_features[i,:]).any():\n",
    "        nan_indices.append(i)\n",
    "pretrained_features = np.delete(pretrained_features, nan_indices, axis=0)\n",
    "tmp_labels = np.delete(labels, nan_indices, axis=0)\n",
    "tmp_positions = np.delete(positions, nan_indices, axis=0)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(pretrained_features, tmp_labels, test_size = 0.2)    \n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=10, \n",
    "    batch_size=32,\n",
    "    validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-cc5aced962c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtmp_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_predicted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtmp_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdouble_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdouble_event_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_positions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/git/master_scripts/data_functions/master_data_functions/functions.py\u001b[0m in \u001b[0;36mdouble_event_distance\u001b[0;34m(results, positions)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;31m#        distances[i] = -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0mdouble_distances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "tmp_predicted = model.predict(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 2)\n",
      "[8.633969e-05 9.999137e-01]\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geir/git/master_analysis/notebooks/pretrained/pretrained_venv/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "print(tmp_predicted.shape)\n",
    "print(tmp_predicted[1])\n",
    "tmp_results = np.where(tmp_predicted[:] != tmp_labels[:], 0, 1)\n",
    "print(tmp_results.shape)\n",
    "#double_events = double_event_distance(tmp_results, tmp_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check predictions\n",
    "predicted = model.predict(x_test)\n",
    "print(len(predicted[np.where(predicted[:,1] == 1.0)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pretrained_venv",
   "language": "python",
   "name": "pretrained_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
