{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module and data import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from master_scripts.data_functions import (load_experiment, get_git_root, separation_distance, energy_difference,\n",
    "                                           relative_energy, event_indices, normalize_image_data)\n",
    "from master_scripts.analysis_functions import (doubles_classification_stats, singles_classification_stats)\n",
    "from sklearn.metrics import f1_score\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "repo_root = get_git_root()\n",
    "\n",
    "images = np.load(repo_root + \"data/simulated/images_full_pixelmod.npy\")\n",
    "positions = np.load(repo_root + \"data/simulated/positions_full.npy\")\n",
    "energies = np.load(repo_root + \"data/simulated/energies_full.npy\")\n",
    "labels = np.load(repo_root + \"data/simulated/labels_full.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Electron Events\n",
    "For this part of the project, the goal is to separate events in the dataset into two categories, 'single' and 'double' electron events. Mainly two paths have been taken in order to do this so far. One using well-known network architectures pretrained on the [ImageNet](http://www.image-net.org/) database. The other using a network architecture developed in an ML-project at MSU, that we train from scratch ([MSU ML-Project, LaBollita](https://github.com/harrisonlabollita/MSU-Machine-Learning-Project)). In both cases the models are deep convolutional neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "## Classification Using Pretrained Models\n",
    "Pretrained networks have previously been found to perform fairly well on other data than they were trained on,\n",
    "such as data from the AT-TPC ([Kuchera et. al](https://arxiv.org/abs/1810.10350)). Because these models are trained on complex image data with a large amount of features, the idea is to use them as \"feature extractors\" on our own image data. Due to modern Python frameworks such as [TensorFlow](https://www.tensorflow.org/), [PyTorch](https://pytorch.org/), and [Keras](https://keras.io/), implementing and testing this approach is fairly straightforward, although not without challenges. The extracted features are then fed through a fully connected\n",
    "network that we build and train from scratch using data with known labels.\n",
    "\n",
    "We attempted classification with the following models, all pretrained on ImageNet:\n",
    "* [DenseNet121, DenseNet169, DensNet201](https://arxiv.org/abs/1608.06993)\n",
    "* [InceptionResNetV2](https://arxiv.org/abs/1602.07261)\n",
    "* [InceptionV3](http://arxiv.org/abs/1512.00567)\n",
    "* [MobileNet](https://arxiv.org/pdf/1704.04861.pdf)\n",
    "* [MobileNetV2](https://arxiv.org/abs/1801.04381)\n",
    "* [NASNetLarge, NASNetMobile](https://arxiv.org/abs/1707.07012)\n",
    "* [ResNet50](https://arxiv.org/abs/1512.03385)\n",
    "* [VGG16, VGG19](https://arxiv.org/abs/1409.1556)\n",
    "* [Xception](https://arxiv.org/abs/1610.02357)\n",
    "\n",
    "See https://keras.io/applications/ for preliminary info about implementation of these models.\n",
    "\n",
    "One aspect of pretrained models is they could allow testing for feasability for ML applications\n",
    "to classification or regression tasks, without needing an expensive infrastructure for training\n",
    "models (looking at you, RTX2080Ti)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges\n",
    "\n",
    "#### Input color\n",
    "The models expect an RGB image as input, meaning dimensions of (height, width, channels), e.g VGG16 has default input (224, 224, 3). We can 'fake' these RGB channels by concatenating our input to itself, making our (16, 16, 1) input (16, 16, 3).\n",
    "\n",
    "#### Input size and MaxPooling layers\n",
    "Our data is essentially a 16x16 pixel image. This is far smaller than the expected input size for most,\n",
    "if not all of these pretrained networks. Without doing any image manipulations, such as padding with additional rows and columns, this stops us from using the full depth of some networks. The typical architectures of these networks contain 'convolutional blocks' followed my a MaxPooling layer. The MaxPooling layers effectively cuts the input size in half for each MaxPooling layer. So for networks where we meet more than three such layers, our input is reduced to a single pixel, and the model will throw an error.\n",
    "\n",
    "To combat this, the implementation of the models does two things:\n",
    "1. Replace the input layer with one that accepts our input, (16, 16, 3). There are no weights in the input layer so this does not affect results.\n",
    "2. Iterate over the layers in the model, adding them to our new model one by one, until we have added all layers or a specific error is thrown. This specific error allows us to catch when our model is too deep, and save the model in the state it is before this error is reached.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are the extracted features viable for classification?\n",
    "Before starting training of fully-connected networks that use the extracted features from pretrained models as input, we compared the feature distributions of each network to see if it is reasonable to expect classification to work. The distribution for each individual feature, across all provided samples, was compared for single and double events by using the [Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test). If the difference between the distributions is significant, there should be a meaningful difference between single and double events that allow classification to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for pretrained models\n",
    "The table below containes the results from our experiments with pretrained networks. The networks were trained on 160000 samples and tested on 40000 samples. The dataset was balanced (same number of each type of event present). To increase the robustness of the results we used k-fold cross-validation with 5 folds for all networks. This gives a clearer picture of the actual performance of the network, because without it you can technically get lucky and only have \"easy\" samples in your validation data the first run and get artificially good results.\n",
    "\n",
    "| Model           | Min Accuracy | Max accuracy | Mean accuracy\n",
    "| :---            |     :---:    |     :---:    |    :---:     \n",
    "|DenseNet121      | 0.92         | 0.93         | 0.92\n",
    "|DenseNet169      | 0.92         | 0.93         | 0.92\n",
    "|DenseNet201      | 0.92         | 0.94         | 0.93\n",
    "|InceptionResNetV2| 0.88         | 0.89         | 0.88\n",
    "|InceptionV3      | 0.87         | 0.88         | 0.88\n",
    "|MobileNet        | 0.50         | 0.86         | 0.71\n",
    "|MobileNetV2      | 0.50         | 0.50         | 0.50\n",
    "|NASNetLarge      | 0.91         | 0.92         | 0.92\n",
    "|NASNetMobile     | 0.91         | 0.92         | 0.92\n",
    "|ResNet50         | 0.50         | 0.50         | 0.50\n",
    "|VGG16            | 0.91         | 0.92         | 0.91\n",
    "|VGG19            | 0.88         | 0.90         | 0.89\n",
    "|Xception         | 0.92         | 0.93         | 0.92\n",
    "\n",
    "The network with the best accuracy was DenseNet201, both for max and mean accuracy. It is closely followed by the other DenseNet variants, NASNet variants, Xception, and VGG16. We chose DenseNet201 for further study. There are a number of other metrics than accuracy that together provide a deeper insight into the performance of a classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet201 Additional Analysis\n",
    "To gain more insight we produce a few more metrics specifically for the top-performing model.\n",
    "These metrics are:\n",
    "* [ROC-Curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n",
    "* [F1-Score](https://en.wikipedia.org/wiki/F1_score)\n",
    "* [Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "\n",
    "Specifically for double events, we look for which type of double events are more difficult to classify than others. To find out, we explore the distribution of relative energy and separation distances for events that are misclassified. We expect to find that low separation distances are more difficult as this is currently the case for humans. Double-events with sufficiently low separation distance may be indistinguishable from single-events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which events are difficult to classify\n",
    "* Plot relative energy without swapping e1 and e2 to constrain values to [0,1]\n",
    "    Done.\n",
    "* Scatterplot correctly classified events as well\n",
    "* Investigate if there are some images that are always misclassified across multiple training runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with custom model\n",
    "We base our implemented model on the structure of state of the art models like VGG,\n",
    "and compare the results with the model from previous work ([MSU ML-Project, LaBollita](https://github.com/harrisonlabollita/MSU-Machine-Learning-Project))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Experiment metrics\n",
      "{\n",
      "  \"accuracy_score\": 0.9859178947368421,\n",
      "  \"confusion_matrix\": {\n",
      "    \"TN\": 236629,\n",
      "    \"FP\": 903,\n",
      "    \"FN\": 5786,\n",
      "    \"TP\": 231682\n",
      "  },\n",
      "  \"f1_score\": 0.9857696898009374,\n",
      "  \"matthews_corrcoef\": 0.9720411813174418,\n",
      "  \"roc_auc_score\": 0.9938551759726573\n",
      "}\n",
      "====\n",
      "F1-score for double events separated by less than 1 pixel: 0.6358333333333334\n",
      "F1-score for double events separated by more than 1 pixel: 0.9893799717573019\n",
      "F1-score for all events: 0.9857696898009374\n"
     ]
    }
   ],
   "source": [
    "# Load experiment and associated model (must be a saved model instance complete with weights)\n",
    "c_experiment_id = \"367e35da671b\"\n",
    "c_experiment = load_experiment(c_experiment_id)\n",
    "c_model = tf.keras.models.load_model(repo_root + \"models/\" + c_experiment_id + \".h5\")\n",
    "# Print experiment metrics\n",
    "print(\"==== Experiment metrics\")\n",
    "print(json.dumps(c_experiment['metrics'], indent=2))\n",
    "print(\"====\")\n",
    "\n",
    "# Get validation indices used in the experiment and predict on validation set\n",
    "c_val_idx = np.array(c_experiment['indices']['fold_0']['val_idx'])\n",
    "# Predict on the validation set\n",
    "c_prediction = c_model.predict(normalize_image_data(images[c_val_idx]))\n",
    "c_val_pred = (c_prediction > 0.5).astype(int)\n",
    "\n",
    "c_s_idx, c_d_idx, c_c_idx = event_indices(positions[c_val_idx])\n",
    "c_non_close_idx = np.setdiff1d(np.concatenate((c_s_idx, c_d_idx), axis=0), c_c_idx)\n",
    "c_f1_close = f1_score(labels[c_val_idx][c_c_idx], c_val_pred[c_c_idx])\n",
    "c_f1_non_close = f1_score(labels[c_val_idx][c_non_close_idx], c_val_pred[c_non_close_idx])\n",
    "print(\"F1-score for double events separated by less than 1 pixel:\", c_f1_close)\n",
    "print(\"F1-score for double events separated by more than 1 pixel:\", c_f1_non_close)\n",
    "print(\"F1-score for all events:\", c_experiment['metrics']['f1_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of Energies and Positions\n",
    "For this part of the project, the goal is to predict the energy and position of electrons in an event in the dataset. Previous work has made predictions of position for single-electron events with great performance ([MSU ML-Project, LaBollita](https://github.com/harrisonlabollita/MSU-Machine-Learning-Project)). We aim to reproduce and possibly improve the results from previous work on position prediction, predict the energy in single-electron events, and then move on to predict positions and energies in double-electron events.\n",
    "\n",
    "This is essentially a regression problem, in that we have a continuous output variable that we want to relate to some input variables (our images). Similarly to classification, the convolutional layer works as a sort of feature extractor. The feature representation of our input is the fed as input to a regression layer which outputs our positions or energies. In fact, you can predict these values using linear regression, it just doesn't work very well. Thus, we enter the realm of \"Deep Regression\".\n",
    "\n",
    "In part we intend to follow the work done in [A Comprehensive Analysis of Deep Regression](https://arxiv.org/abs/1803.08450), but using our own models built from scratch. Using pretrained networks is a possible path to try here, but currently we've met with implementation difficulties due to the size of our input.\n",
    "We might solve this the same way as for classification, by simply not using all the layers. However, the article above also found that the placement of the regression layer was crucial, and performed best when placed after both fully connected layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-electron events\n",
    "Based on the architecture developed in previous work, we have made two separate models for prediction of position and energy. However, the only difference between the models for the single-electron case is the final output layer, which must account for outputting either one value in the energy case, or two values in the position case (x, y coordinates)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy prediction results\n",
    "500k events, R2 = 0.9764 after 4 epochs (earlystopping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Experiment metrics\n",
      "{\n",
      "  \"r2_score\": 0.9826797076330008,\n",
      "  \"mse\": 0.001442644048834561,\n",
      "  \"rmse\": 0.037982154346937205,\n",
      "  \"mae\": 0.0174867983148799\n",
      "}\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "# Load experiment and associated model (must be a saved model instance complete with weights)\n",
    "e_experiment_id = \"2137bd6d101c\"\n",
    "e_experiment = load_experiment(e_experiment_id)\n",
    "e_model = tf.keras.models.load_model(repo_root + \"models/\" + e_experiment_id + \".h5\")\n",
    "# Print experiment metrics\n",
    "print(\"==== Experiment metrics\")\n",
    "print(json.dumps(e_experiment['metrics'], indent=2))\n",
    "print(\"====\")\n",
    "\n",
    "# Get validation indices used in the experiment and predict on validation set\n",
    "#e_val_idx = np.array(e_experiment['indices']['fold_0']['val_idx'])\n",
    "# Predict on the validation set\n",
    "#e_prediction = e_model.predict(normalize_image_data(images[e_val_idx]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position prediction results\n",
    "500k events, R2 = 0.9855 after 5 epochs (earlystopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Experiment metrics\n",
      "{\n",
      "  \"r2_score\": 0.9991389940117517,\n",
      "  \"mse\": 6.012267605743713e-05,\n",
      "  \"rmse\": 0.007753881354356483,\n",
      "  \"mae\": 0.003687259271068965\n",
      "}\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "# Load experiment and associated model (must be a saved model instance complete with weights)\n",
    "p_experiment_id = \"337cafc233f7\"\n",
    "p_experiment = load_experiment(p_experiment_id)\n",
    "p_model = tf.keras.models.load_model(repo_root + \"models/\" + p_experiment_id + \".h5\")\n",
    "# Print experiment metrics\n",
    "print(\"==== Experiment metrics\")\n",
    "print(json.dumps(p_experiment['metrics'], indent=2))\n",
    "print(\"====\")\n",
    "\n",
    "# Get validation indices used in the experiment and predict on validation set\n",
    "#p_val_idx = np.array(p_experiment['indices']['fold_0']['val_idx'])\n",
    "# Predict on the validation set\n",
    "#p_prediction = p_model.predict(normalize_image_data(images[p_val_idx]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double-electron events\n",
    "Based on the architecture used in classification, we have made two separate models for prediction of position and energy. This is also partly because of the difference in output, but also because we assume the tasks of predicting these quantities may be fundamentally different. It is also trivial to attempt prediction of positions using the energy model (and vice versa), as long as the final output layer is adjusted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy prediction results\n",
    "500k events, R2 = 0.4552 (3 epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position prediction results\n",
    "500k events, R2 = 0.4819 after 3 epochs (earlystopping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why the low score compared with single event?\n",
    "* Loss functions\n",
    "* Target and input representation\n",
    "* Convolution blocks - complexity of network\n",
    "* Fine-Tuning (ref. article, on pretrained networks)\n",
    "* Regression layer placement\n",
    "* Is the CNNs spacial invariance affecting its ability to predict positions well in multiple-object cases?\n",
    "\n",
    "Papers to check out:\n",
    "* [Numerical Coordinate Regression with CNNs](https://arxiv.org/abs/1801.07372)\n",
    "* [DeepDistance](https://arxiv.org/abs/1908.11211)\n",
    "* [A Comprehensive Analysis of Deep Regression](https://arxiv.org/abs/1803.08450)\n",
    "* [Human pose estimation via Convolutional Part Heatmap Regression](https://arxiv.org/abs/1609.01743)\n",
    "* [Evaluating and Calibrating Uncertainty Prediction in Regression Tasks](https://arxiv.org/abs/1905.11659)\n",
    "\n",
    "\n",
    "Group by distance to check if there are some events that are dominating the results.\n",
    "Also for relative energies. \n",
    "* Can we treat the data as heatmaps? Or can we output heatmaps from the CNN?\n",
    "* Look at object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability of models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME\n",
    "LIME (Local Interpretable Model-Agnostic Explanations, [Module](https://github.com/marcotcr/lime), [Paper](https://arxiv.org/abs/1602.04938)) is a project that aims to explain what classifiers are doing.\n",
    "Have done preliminary testing with classification, but:\n",
    "* Must make some changes to make it run with our classification model. Seems to be a problem with LIME assuming 3 channels (RGB).\n",
    "\n",
    "If we can explain the behaviour of the classifier to a greater extent, perhaps this can aid in the regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Methods\n",
    "Unsupervised methods do not require labeled data, but rather look for underlying symmetries and structures in the data itself.\n",
    "\n",
    "Currently looking into:\n",
    "* Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline for thesis\n",
    "Start with the standard framework, similar to projects.\n",
    "## Abstract\n",
    "## Introduction\n",
    "### Motivate the reader, why is this project relevant\n",
    "## Theory\n",
    "### LinReg\n",
    "### LogReg\n",
    "### NNs, backprop\n",
    "### Convolutions\n",
    "## Implementation\n",
    "### Testing and training with simulated data\n",
    "### Examples with current methods"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "318.003px",
    "width": "271.788px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1094px",
    "left": "229px",
    "top": "110px",
    "width": "254.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
