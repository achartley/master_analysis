{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Electron Events\n",
    "For this part of the project, the goal is to separate events in the dataset into two categories, 'single' and 'double' electron events. Mainly two paths have been taken in order to do this so far. One using well-known network architectures pretrained on the [ImageNet](http://www.image-net.org/) database. The other using a network architecture developed in an ML-project at MSU, that we train from scratch ([MSU ML-Project, LaBollita](https://github.com/harrisonlabollita/MSU-Machine-Learning-Project)). In both cases the models are deep convolutional neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "## Classification Using Pretrained Models\n",
    "Pretrained networks have previously been found to perform fairly well on other data than they were trained on,\n",
    "such as data from the AT-TPC ([Kuchera et. al](https://arxiv.org/abs/1810.10350)). Because these models are trained on complex image data with a large amount of features, the idea is to use them as \"feature extractors\" on our own image data. Due to modern Python frameworks such as [TensorFlow](https://www.tensorflow.org/), [PyTorch](https://pytorch.org/), and [Keras](https://keras.io/), implementing and testing this approach is fairly straightforward, although not without challenges. The extracted features are then fed through a fully connected\n",
    "network that we build and train from scratch using data with known labels.\n",
    "\n",
    "We attempted classification with the following models, all pretrained on ImageNet:\n",
    "* [DenseNet121, DenseNet169, DensNet201](https://arxiv.org/abs/1608.06993)\n",
    "* [InceptionResNetV2](https://arxiv.org/abs/1602.07261)\n",
    "* [InceptionV3](http://arxiv.org/abs/1512.00567)\n",
    "* [MobileNet](https://arxiv.org/pdf/1704.04861.pdf)\n",
    "* [MobileNetV2](https://arxiv.org/abs/1801.04381)\n",
    "* [NASNetLarge, NASNetMobile](https://arxiv.org/abs/1707.07012)\n",
    "* [ResNet50](https://arxiv.org/abs/1512.03385)\n",
    "* [VGG16, VGG19](https://arxiv.org/abs/1409.1556)\n",
    "* [Xception](https://arxiv.org/abs/1610.02357)\n",
    "\n",
    "See https://keras.io/applications/ for preliminary info about implementation of these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges\n",
    "\n",
    "#### Input color\n",
    "The models expect an RGB image as input, meaning dimensions of (height, width, channels), e.g VGG16 has default input (224, 224, 3). We can 'fake' these RGB channels by concatenating our input to itself, making our (16, 16, 1) input (16, 16, 3).\n",
    "\n",
    "#### Input size and MaxPooling layers\n",
    "Our data is essentially a 16x16 pixel image. This is far smaller than the expected input size for most,\n",
    "if not all of these pretrained networks. Without doing any image manipulations, such as padding with additional rows and columns, this stops us from using the full depth of some networks. The typical architectures of these networks contain 'convolutional blocks' followed my a MaxPooling layer. The MaxPooling layers effectively cuts the input size in half for each MaxPooling layer. So for networks where we meet more than three such layers, our input is reduced to a single pixel, and the model will throw an error.\n",
    "\n",
    "To combat this, the implementation of the models does two things:\n",
    "1. Replace the input layer with one that accepts our input, (16, 16, 3). There are no weights in the input layer so this does not affect results.\n",
    "2. Iterate over the layers in the model, adding them to our new model one by one, until we have added all layers or a specific error is thrown. This specific error allows us to catch when our model is too deep, and save the model in the state it is before this error is reached.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are the extracted features viable for classification?\n",
    "Before starting training of fully-connected networks that use the extracted features from pretrained models as input, we compared the feature distributions of each network to see if it is reasonable to expect classification to work. The distribution for each individual feature, across all provided samples, was compared for single and double events by using the [Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test). If the difference between the distributions is significant, there should be a meaningful difference between single and double events that allow classification to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for pretrained models\n",
    "The table below containes the results from our experiments with pretrained networks. The networks were trained on 160000 samples and tested on 40000 samples. The dataset was balanced (same number of each type of event present). To increase the robustness of the results we used k-fold cross-validation with 5 folds for all networks. This gives a clearer picture of the actual performance of the network, because without it you can technically get lucky and only have \"easy\" samples in your validation data the first run and get artificially good results.\n",
    "\n",
    "| Model           | Min Accuracy | Max accuracy | Mean accuracy\n",
    "| :---            |     :---:    |     :---:    |    :---:     \n",
    "|DenseNet121      | 0.92         | 0.93         | 0.92\n",
    "|DenseNet169      | 0.92         | 0.93         | 0.92\n",
    "|DenseNet201      | 0.92         | 0.94         | 0.93\n",
    "|InceptionResNetV2| 0.88         | 0.89         | 0.88\n",
    "|InceptionV3      | 0.87         | 0.88         | 0.88\n",
    "|MobileNet        | 0.50         | 0.86         | 0.71\n",
    "|MobileNetV2      | 0.50         | 0.50         | 0.50\n",
    "|NASNetLarge      | 0.91         | 0.92         | 0.92\n",
    "|NASNetMobile     | 0.91         | 0.92         | 0.92\n",
    "|ResNet50         | 0.50         | 0.50         | 0.50\n",
    "|VGG16            | 0.91         | 0.92         | 0.91\n",
    "|VGG19            | 0.88         | 0.90         | 0.89\n",
    "|Xception         | 0.92         | 0.93         | 0.92\n",
    "\n",
    "The network with the best accuracy was DenseNet201, both for max and mean accuracy. It is closely followed by the other DenseNet variants, NASNet variants, Xception, and VGG16. We chose DenseNet201 for further study. There are a number of other metrics than accuracy that together provide a deeper insight into the performance of a classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet201 Additional Analysis\n",
    "To gain more insight we produce a few more metrics specifically for the top-performing model.\n",
    "These metrics are:\n",
    "* [ROC-Curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n",
    "* [F1-Score](https://en.wikipedia.org/wiki/F1_score)\n",
    "* [Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "\n",
    "Specifically for double events, we look for which type of double events are more difficult to classify than others. To find out, we explore the distribution of relative energy and separation distances for events that are misclassified. We expect to find that low separation distances are more difficult as this is currently the case for humans. Double-events with sufficiently low separation distance may be indistinguishable from single-events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which events are difficult to classify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of Energies and Positions\n",
    "For this part of the project, the goal is to predict the energy and position of electrons in an event in the dataset. Previous work has made predictions of position for single-electron events with great performance ([MSU ML-Project, LaBollita](https://github.com/harrisonlabollita/MSU-Machine-Learning-Project)). We aim to reproduce and possibly improve the results from previous work on position prediction, predict the energy in single-electron events, and then move on to predict positions and energies in double-electron events.\n",
    "\n",
    "This is essentially a regression problem, in that we have a continuous output variable that we want to relate to some input variables (our images). Similarly to classification, the convolutional layer works as a sort of feature extractor. The feature representation of our input is the fed as input to a regression layer which outputs our positions or energies. In fact, you can predict these values using linear regression, it just doesn't work very well. Thus, we enter the realm of \"Deep Regression\".\n",
    "\n",
    "In part we intend to follow the work done in [A Comprehensive Analysis of Deep Regression](https://arxiv.org/abs/1803.08450), but using our own models built from scratch. Using pretrained networks is a possible path to try here, but currently we've met with implementation difficulties due to the size of our input.\n",
    "We might solve this the same way as for classification, by simply not using all the layers. However, the article above also found that the placement of the regression layer was crucial, and performed best when placed after both fully connected layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-electron events\n",
    "Based on the architecture developed in previous work, we have made two separate models for prediction of position and energy. However, the only difference between the models for the single-electron case is the final output layer, which must account for outputting either one value in the energy case, or two values in the position case (x, y coordinates)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy prediction results\n",
    "500k events, R2 = 0.9764 after 4 epochs (earlystopping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position prediction results\n",
    "500k events, R2 = 0.9855 after 5 epochs (earlystopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double-electron events\n",
    "Based on the architecture used in classification, we have made two separate models for prediction of position and energy. This is also partly because of the difference in output, but also because we assume the tasks of predicting these quantities may be fundamentally different. It is also trivial to attempt prediction of positions using the energy model (and vice versa), as long as the final output layer is adjusted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy prediction results\n",
    "500k events, R2 = 0.4552 (3 epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position prediction results\n",
    "500k events, R2 = 0.4819 after 3 epochs (earlystopping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability of models\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "summary",
   "language": "python",
   "name": "summary"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "318.003px",
    "width": "271.788px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
